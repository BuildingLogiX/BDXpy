{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to BDXpy Documentation","text":"<p> <code>bdxpy</code> is a Python package designed to interface with BuildingLogiX Data Exchange (BDX), a building analytics platform developed by BuildingLogiX.\ud83d\udcca</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Access BDX trends and analytics: Enables access to building information, trended data, equipment scores and energy analytics.</li> <li>Handles BDX authentication: Hides the complexity of BDX authentication and authorization mechanisms.</li> <li>Provides data in native format: Data series retrieved from BDX are available in Pandas DataFrame format, which enables intuitive processing in Python.</li> </ul> <p>Get started with installation instructions.</p>"},{"location":"#summary-overview-and-concepts","title":"Summary Overview and Concepts","text":"<p>This package allows users to interact with an API hosted on a BDX site. BDXpy is a Python library designed to facilitate interaction with BuildingLogiX's API for managing building data, trends, components, and hierarchies. This guide provides an overview of the library's features, including how to authenticate, retrieve data, and handle exceptions. It includes modules for authentication, session management, trend data retrieval, and component lookup. Our goals with BDXpy are:</p> <ul> <li>\ud83d\udd25 Allow a new canvas of opportunities for partners \u2013 almost endless with python and package features.</li> <li>\ud83d\ude80Quicker development ideas with examples from partners and internal BLX teams\ud83d\ude80</li> <li>More \u201cpower\u201d or experimentation on the end user.\u2714\ufe0f</li> <li>Take advantage of Open-Source capabilities and help \ud83d\udca1define what the future of FDD and Optimization could become at a lower cost.\ud83d\udca1</li> <li>\ud83d\udc4d Promote more advanced Machine Learning and Energy savings outcomes and grow the industry.</li> <li>\u2728\u2728Create a library/standard of advanced visualization and analysis available to the greater community.\u2728\u2728</li> </ul>"},{"location":"#references","title":"References","text":"<ul> <li>https://buildinglogix.github.io/BDXpy/</li> <li>https://pypi.org/project/bdxpy/</li> <li>https://buildinglogix.net/</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<ul> <li>Automated custom reports</li> <li>Machine Learning Applications</li> <li>\ud83e\udd16 AI integrations (OpenAI, Gemini, etc.)</li> <li>Advanced/Custom Dashboards and Visualizations \ud83d\udcc9\ud83d\udcc9\ud83d\udcc9\ud83d\udcc9</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-to-bdxpy","title":"Contributing to BDXpy","text":"<p>We welcome and encourage participation in the BDXpy community! While direct code contributions are not required, here\u2019s how you can get involved:</p>"},{"location":"contributing/#share-your-creations","title":"\ud83d\udcca Share Your Creations","text":"<p>Showcase your charts, visualizations, and analyses in the GitHub Discussions section under User Creations. Upload images, share insights, and discuss results with the community.</p>"},{"location":"contributing/#ask-questions-troubleshoot","title":"\u2753 Ask Questions &amp; Troubleshoot","text":"<p>If you have questions about BDXpy, visit the Q&amp;A section in Discussions to get help. Check the Issues tab to see if your question has already been answered or report a bug.</p>"},{"location":"contributing/#reporting-issues","title":"\ud83d\udc1e Reporting Issues","text":"<p>Found a bug? Open a new issue in the Issues section with as much detail as possible. If applicable, include screenshots, error messages, or a minimal dataset to reproduce the issue.</p>"},{"location":"contributing/#suggest-features-improvements","title":"\ud83d\udca1 Suggest Features &amp; Improvements","text":"<p>Have an idea to improve BDXpy? Share it in Discussions</p> <p>\ud83d\ude80 Thank you for being a part of the BDXpy community! Your engagement helps improve the project for everyone. \ud83c\udf89</p>"},{"location":"examples/","title":"\ud83d\udcca Example Highlights Gallery","text":"<p>Click in the left navigation for more details on specific examples</p>"},{"location":"examples/#data-visualization","title":"\ud83d\udd25 Data Visualization","text":"<ul> <li> <p> Airflow Sankey</p> <p></p> </li> <li> <p> Building CFM Network</p> <p></p> </li> <li> <p> Campus Energy Map</p> <p></p> </li> <li> <p> AHU Economizer Analysis</p> <p></p> </li> <li> <p> Operating Room KPIs</p> <p></p> </li> <li> <p> Static Pressure Distribution</p> <p></p> </li> </ul>"},{"location":"examples/#dashboards","title":"Dashboards","text":"<ul> <li> RTU Dashboard</li> </ul> \ud83d\udd0d Click for Fullscreen"},{"location":"examples/#machine-learning","title":"Machine Learning","text":"<p>More examples coming soon!</p>"},{"location":"examples/#public-kiosk-app","title":"Public Kiosk App","text":"<p>More examples coming soon!</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#how-do-i-install-bdxpy","title":"How do I install BDXpy?","text":"<p>Use pip: <pre><code>pip install bdxpy\n</code></pre> See the installation section.</p>"},{"location":"faq/#how-do-use-bdxpy","title":"How do use BDXpy?","text":"<p>See our workflow diagram for getting started: workflow bdxpy.</p>"},{"location":"faq/#do-i-have-to-pay-for-bdxpy","title":"Do I have to pay for BDXpy?","text":"<p>BDXpy requires an active BDX license including the license feature for RemoteAPI. Installation of the Python package BDXpy is free and publicly available as well as all documentation and some basic examples to get users started.</p>"},{"location":"faq/#what-if-i-dont-know-python","title":"What if I don't know Python?","text":"<p>Python is one of the most widely used programming languages and is open-source. It has many resources, classes, examples online beginners can pull from. Likewise most AI chatbots like ChatGPT, Gemini, etc. all generally quite good a Python development and we highly encourage beginners to utilize all these available tools.  </p> <p>BuildingLogiX is also working on forming a dedicated BDXpy custom GPT, coming soon.....</p>"},{"location":"faq/#what-are-ides-which-one-should-i-use-for-bdxpy","title":"What are IDEs, which one should I use for BDXpy?","text":"<p>An IDE (Integrated Development Environment) is a software application that provides coding tools, such as syntax highlighting, debugging, and auto-completion, to make development easier.</p> <p>For working with BDXpy, most of our team uses VS Code most of the time or Jupyter Notebook in VS Code. There are others we also recommend depending on your experience and preference:</p> <ul> <li>VS Code \u2013 Lightweight, customizable, and great for Python development.</li> <li>PyCharm \u2013 More feature-rich, with advanced debugging and code analysis.</li> <li>Jupyter Notebook \u2013 Best for interactive data analysis and quick visualizations.</li> <li>Spyder \u2013 Designed for data science, similar to MATLAB.  </li> </ul> <p>\ud83d\udc49 Recommendation: If you need a full-fledged Python IDE, VS Code or PyCharm is great. If you are doing data exploration, try Jupyter Notebook.</p>"},{"location":"faq/#what-is-the-difference-between-python-and-jupyter-notebook-files","title":"What is the difference between Python and Jupyter Notebook files?","text":"<ul> <li>Python scripts (.py):<ul> <li>Standard Python files used for writing and executing programs. Run using a terminal, command line, or an IDE like VS Code/PyCharm. Best for larger projects and production-ready code.</li> </ul> </li> <li>Jupyter Notebooks (.ipynb):<ul> <li>An interactive environment that allows you to mix code, markdown, and visualizations. Best for data analysis, quick prototyping, and educational purposes. Supports inline plots (like matplotlib and seaborn).  </li> </ul> </li> </ul> <p>\ud83d\udc49 Recommendation: Use .py for software development and .ipynb for analysis, documentation, or interactive work.</p>"},{"location":"faq/#can-buildinglogix-help-me-develop-in-python","title":"Can BuildingLogiX help me develop in Python?","text":"<p>Yes, BuildingLogiX has in-house developers and engineers who are happy to assist. IF you want to collarborate on BDXpy development it is considered custom development and involves working directly with our team! We will also likely have a series of small training seminars annual on BDXpy. Reach out to us if you are interested.</p>"},{"location":"faq/#who-can-i-contact-for-support","title":"Who can I contact for support?","text":"<p>Email us at technical.support@buildinglogix.net.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#applications-and-use-cases","title":"Applications and Use Cases","text":"<p>BDXpy is a powerful companion tool for use with BDX installations. This tool designed for advanced users who need to interface with the BuildingLogiX Data Exchange (BDX) platform for retrieving, processing, and analyzing building performance data in a complex/customized way. While we prefer to develop all great ideas into the main BDX development roadmap, BDXpy gives use the ability NOW to bring their own ideas to life or create value in ways that might not make sense as a one off project in BDX.</p> <p>Some key applications include:</p> <ul> <li>As a Data Playground \u2013 Automate the retrieval of historical and real-time energy consumption data for custom reporting and analysis.</li> <li>As a Data Service \u2013 run BDXpy inside automated reports or on servers as an API service. For example in Grafana dashboards or weekly energy/FDD summaries of your buildings major KPIs.</li> <li>As an Application \u2013 Build interactive public dashboards for stakeholder using Flask, Dash, or other web frameworks.</li> <li>As a Solution \u2013 Extract data from BDX and integrate it with other platforms, including cloud databases, CMMS, and BI tools. Maybe create automated analysis and summaries for issues with AI assitants??</li> </ul> <p>Note: BDXpy development is custom to each end user and each BDX site. The responsibility of best practices around coding, data retrievals, architecture, and deployment practices are yours. Unless contracted BuildingLogiX is not responsible nor warrants user created scripts, programs, etc.</p> <p>BuildingLogiX is providing a pathway for advanced capabilities and a gateway to large volumes of organized data. If you wish to have a consultant or use BuildingLogiX in your development with BDXpy we are happy to engage members of our development and engineering teams. Reach us at technical.support@buildinglogix.net</p> <p></p>"},{"location":"getting-started/#why-python","title":"Why Python?","text":"<p>BDXpy was built using Python because of its flexibility, powerful capabilities, and vast ecosystem of open-source libraries. Python is widely used in data science, automation, and web applications, making it an ideal choice for interacting with the BuildingLogiX Data Exchange (BDX) platform.</p>"},{"location":"getting-started/#key-reasons-for-choosing-python","title":"Key Reasons For Choosing Python:","text":"<ul> <li>Ease of Use &amp; Readability \u2013 Python's simple syntax makes it accessible for both beginners and advanced users, allowing for rapid development and maintainability.</li> <li>Large Open-Source Community \u2013 Python has one of the largest and most active developer communities, ensuring extensive support, tutorials, and libraries for virtually any task.</li> <li>Rich Data Science Ecosystem \u2013 Libraries like pandas, NumPy, and Matplotlib enable powerful data manipulation, statistical analysis, and visualization.</li> <li>Scalability &amp; Performance \u2013 Python can be used for quick prototyping as well as large-scale applications, supporting both small scripts and enterprise-level solutions.</li> <li>Extensive Integration Options \u2013 Python easily integrates with databases, web services, cloud platforms (AWS, Azure, GCP), and APIs, making it the perfect language for automation and analytics.</li> <li>Cross-Platform Compatibility \u2013 Python runs on Windows, macOS, and Linux, ensuring that BDXpy can be deployed across different environments with minimal setup.</li> <li>Automation &amp; Scripting \u2013 Python excels at automating repetitive tasks, scheduling reports, and managing data pipelines with minimal effort.</li> </ul> <p>By leveraging Python\u2019s strengths, BDXpy provides a robust, scalable, and easy-to-use interface for working with BDX, empowering users to extract insights, automate processes, and build custom applications.</p>"},{"location":"getting-started/#architecture-options","title":"Architecture Options","text":"<p>There are a couple different ways to run/deploy BDXpy depending on your needs and use cases. Technically multiple sources of BDXpy could be running simultaneously performing various tasks. Disclaimer: BDXpy is intended for power users and rates of data collection should be reasonable and if too many requests of large quantities of data  are simultaneously taking place it could strain the BDX application and database servers as well as the local machine hosting BDXpy as large data frames can become memory intensive. BDXpy limitations are dependent both on the location of its operation but also the BDX instance it communicates with. It\u2019s the responsibility of the user to make smart decisions with this knowledge. BDXpy (as the below image suggests) can be used on an existing BDX server, on a dedicated server, or run from a laptop/computer. In all cases the network path between the BDX instance and BDXpy needs to be active for scripts and processes using BDXpy to work.</p>"},{"location":"getting-started/#deployment-cases","title":"Deployment Cases","text":"<ol> <li>On a laptop/personal computer: for engineers wanting to experiment, run reports from their machine, non-24/7 script deployments.<ul> <li>Easiest way to get started or pilot something before deploying elsewhere so long as user has access to BDX from laptop</li> </ul> </li> <li>On an existing BDX server: likely for an on-premises installation where data is served back to BDX via Windows IIS and are publishing BDXpy internally with identical firewall rules to BDX.<ul> <li>Install here if you want BDXpy files to feedback to the kiosk or custom apps/widgets. Maybe require IT approval or cooperation</li> </ul> </li> <li>On a dedicated BDXpy server: user who might have a cloud BDX but locally want to serve up private network, but internal (non-authenticated) dashboard/visuals/kiosks on a local network. Or users who have more powerful tasks or server resources or teams that shouldn\u2019t/can\u2019t access the BDX server RDP directly or who want to have scripts automatically run on a timed/automated schedule<ul> <li>Dedicated servers could also be cloud VMs if they have access to BDX where automated reports or new apps are created. Doesn\u2019t require extra IT permissions or resources on the BDX server depending on the scope.</li> </ul> </li> <li>Cloud Functions: BDXpy scripts can be run in serverless environments such as AWS Lambda or Azure Functions provided, they don\u2019t entail long running or complex processes or other serverless constraints and have access to communicate with the BDX instance/server. See cloud provider for details</li> </ol>"},{"location":"getting-started/#process-overview-from-bdx-to-bdxpy-and-python-outputs","title":"Process Overview \u2013 From BDX to BDXpy and Python Outputs","text":""},{"location":"getting-started/#checking-the-bdx-license-for-bdxpy","title":"Checking the BDX license for BDXpy","text":"<p>BDXpy requires a license feature on the source BDX. These can be viewed in a BDX instance under Manage Licenses to ensure the feature is enabled as in the image below.\u2705</p> <p></p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>Installing the BDX Python package is relatively simple. The first thing required is to download the Python language onto your device.</p> <ol> <li>BDXpy requires Python 3.12 or newer versions (newer versions are recommended). This can be done through the Python downloads website.</li> <li>Ensure BDX instance is licensed for API access.</li> <li>After installing the necessary packages and obtaining the correct licensing, install BDXpy just like any other Python package:</li> </ol> <p><pre><code>pip install bdxpy\n</code></pre> Information on pip here.</p>"},{"location":"getting-started/#requirements-packaged-that-come-with-bdxpy","title":"Requirements packaged that come with BDXpy","text":"<p>BDXpy requires Python 3.12+ and the following dependencies which will be installed with BDXpy:</p> <ul> <li>requests</li> <li>pandas</li> <li>nacl</li> <li>urllib3</li> <li>dacite</li> </ul> <p>Note: you do not need to install these separately as they come with BDXpy, this is purely for reference.</p>"},{"location":"getting-started/#bdxpy-imports","title":"BDXpy Imports","text":"<p>To make use of bdxpy functions python will need \u201cimports\u201d from the package. These imports reside before the code/functions are called in the python script and are all typically handled in the beginning lines of the code like below:</p> <pre><code>from bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame\nfrom bdx.components import Components\n\n## examples of other python package imports (but not limited to) that can be used with data from BDXpy\nfrom datetime import datetime, timedelta\nimport os\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\n</code></pre>"},{"location":"getting-started/#bdxpy-license","title":"BDXpy License","text":"<p>This package is distributed under a proprietary license. Contact BuildingLogiX for licensing information.</p> <p>Learn more about BuildingLogiX Data Exchange (BDX) here. BDXpy on pypi.</p>"},{"location":"pythonpackages/","title":"Python Packages for BDXpy","text":"<p>BDXpy users can benefit from a variety of Python packages to enhance data analysis, visualization, automation, and machine learning. Python has numerous packages than what is listed below for almost anything. Youtube, Google, and other search options can help provide a lot of free examples to beginners. For those getting started or unfamiliar below are some packages that you might find useful working with BDXpy:</p>"},{"location":"pythonpackages/#data-analysis-processing","title":"\ud83d\udcca Data Analysis &amp; Processing","text":""},{"location":"pythonpackages/#pandas-docs","title":"Pandas (Docs)","text":"<ul> <li>Essential for data manipulation and analysis, Pandas makes it easy to handle time-series data retrieved from BDX.</li> <li>Supports powerful data aggregation, filtering, and statistical operations.</li> </ul>"},{"location":"pythonpackages/#numpy-docs","title":"NumPy (Docs)","text":"<ul> <li>Provides fast array operations, mathematical functions, and numerical processing capabilities.</li> <li>Helpful for working with large datasets from BDX efficiently.</li> </ul>"},{"location":"pythonpackages/#reportlab-docs","title":"ReportLab (Docs)","text":"<ul> <li>A powerful library for generating PDFs from data.</li> <li>Useful for exporting BDX reports, trend summaries, and energy analysis into professional-grade PDFs.</li> </ul>"},{"location":"pythonpackages/#data-visualization","title":"\ud83d\udcc8 Data Visualization","text":""},{"location":"pythonpackages/#plotly-docs","title":"Plotly (Docs)","text":"<ul> <li>Ideal for creating interactive charts, including line charts, scatter plots, heatmaps, and dashboards.</li> <li>Works well with BDX trend data and supports rendering inside web applications.</li> </ul>"},{"location":"pythonpackages/#dash-docs","title":"Dash (Docs)","text":"<ul> <li>A great framework for building interactive web applications without needing JavaScript.</li> <li>Useful for creating dynamic visualizations with BDX data.</li> </ul>"},{"location":"pythonpackages/#matplotlib-docs","title":"Matplotlib (Docs)","text":"<ul> <li>A powerful library for static, animated, and interactive visualizations.</li> <li>Useful for quick visual analysis of BDX data without interactive elements.</li> </ul>"},{"location":"pythonpackages/#seaborn-docs","title":"Seaborn (Docs)","text":"<ul> <li>Enhances Matplotlib with advanced statistical visualizations and better default aesthetics.</li> <li>Great for analyzing trends in energy usage and equipment performance.</li> </ul>"},{"location":"pythonpackages/#plotly-mapbox-docs","title":"Plotly Mapbox (Docs)","text":"<ul> <li>Enables map-based visualization of BDX data, such as energy consumption across multiple buildings.</li> <li>Supports overlaying real-time data on campus maps.</li> </ul>"},{"location":"pythonpackages/#machine-learning-forecasting","title":"\ud83d\udd2e Machine Learning &amp; Forecasting","text":""},{"location":"pythonpackages/#scikit-learn-docs","title":"Scikit-learn (Docs)","text":"<ul> <li>A machine learning library with tools for regression, classification, clustering, and anomaly detection.</li> <li>Can be used to analyze energy trends, detect anomalies, and forecast consumption.</li> </ul>"},{"location":"pythonpackages/#xgboost-docs","title":"XGBoost (Docs)","text":"<ul> <li>A high-performance machine learning library optimized for structured data.</li> <li>Useful for predictive modeling of energy consumption and HVAC system behaviors.</li> </ul>"},{"location":"pythonpackages/#prophet-docs","title":"Prophet (Docs)","text":"<ul> <li>A time-series forecasting tool developed by Facebook.</li> <li>Useful for predicting energy usage patterns and trends in BDX.</li> </ul>"},{"location":"pythonpackages/#statsmodels-docs","title":"Statsmodels (Docs)","text":"<ul> <li>Provides statistical models, hypothesis testing, and forecasting.</li> <li>Useful for analyzing historical building energy consumption and HVAC efficiency.</li> </ul>"},{"location":"pythonpackages/#pycaret-docs","title":"PyCaret (Docs)","text":"<ul> <li>A low-code machine learning library that automates model training, selection, and tuning.</li> <li>Great for quickly developing predictive models for BDX data.</li> </ul>"},{"location":"pythonpackages/#automation-optimization","title":"\ud83d\udee0 Automation &amp; Optimization","text":""},{"location":"pythonpackages/#networkx-docs","title":"NetworkX (Docs)","text":"<ul> <li>Helps model and analyze complex relationships within BDX hierarchical components.</li> <li>Useful for understanding interdependencies between buildings, sensors, and control points.</li> </ul>"},{"location":"pythonpackages/#pulp-docs","title":"PuLP (Docs)","text":"<ul> <li>A linear programming and optimization library.</li> <li>Can optimize HVAC schedules or chiller plant operations.</li> </ul>"},{"location":"pythonpackages/#schedule-docs","title":"Schedule (Docs)","text":"<ul> <li>A lightweight job scheduling library for Python.</li> <li>Useful for automating BDX data retrieval and reporting.</li> </ul>"},{"location":"pythonpackages/#flask-docs","title":"Flask (Docs)","text":"<ul> <li>A lightweight web framework for building BDX-powered APIs and dashboards.</li> <li>Can serve energy reports, analytics, and visualizations.</li> </ul>"},{"location":"pythonpackages/#fastapi-docs","title":"FastAPI (Docs)","text":"<ul> <li>A high-performance web framework for building APIs with Python.</li> <li>Faster and more scalable than Flask, making it great for handling large BDX requests.</li> </ul> <p>\ud83d\ude80 Got more package recommendations? Let us know on the discussion board User Creations</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#authentication-and-session-setup","title":"Authentication and Session Setup","text":""},{"location":"reference/#using-a-env-file-for-credentials","title":"Using a .env File for Credentials","text":"<p>To enhance security, you can store the username and password in a .env file rather than hardcoding them into your python code.  Note: Files containing the credentials should have appropriate security practices in place such as user/server permissions so they cannot be accessed/stolen/or compromised.  Below are the examples to set up and use a .env file for authentication. More on this process and security can also be found online.</p> <ol> <li>Create a .env File In your project directory, create a file named .env with the following content: <pre><code>BDX_USERNAME=your_username\nBDX_PASSWORD=your_password\n</code></pre></li> <li>Install the python-dotenv Library Install the library to load environment variables from the .env file: <pre><code>pip install python-dotenv\n</code></pre></li> <li>Load Environment Variables in Your Script Use the following code to load the .env file and authenticate with BDX:</li> </ol> <pre><code>from dotenv import load_dotenv\nimport os\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\n\n# Specify the path to the environment variable file\nload_dotenv(dotenv_path=r\"C:\\Users\\your-path\\.env\")\n\nusername = os.getenv(\"BDX_USERNAME\")\npassword = os.getenv(\"BDX_PASSWORD\")\n\nif not username or not password:\n    raise EnvironmentError(\"Environment variables BDX_USERNAME and BDX_PASSWORD must be set in the .env file\")\n\nauthenticator = UsernameAndPasswordAuthenticator(username, password)\nwith BDX(\"http://yourBDXURL.com\", authenticator) as bdx_instance:\n\n    print(\"Connection successful!\") \n</code></pre>"},{"location":"reference/#credentials-in-script","title":"Credentials in script","text":"<p>For testing purposes the BDX_USERNAME and BDX_PASSWORD can also be copied into the authentication in python.</p> <pre><code>with BDX(\"http://yourBDXURL.com\", UsernameAndPasswordAuthenticator(\n  \"BDX_USERNAME\", \"BDX_PASSWORD\")) as bdx_instance:\n</code></pre>"},{"location":"reference/#important-notes-on-authentication","title":"Important Notes on Authentication","text":"<ul> <li>Currently, the authentication method requires the user to be a BDX local user or an LDAP user. Users authenticated via Single Sign-On (SSO) or Multi-Factor Authentication (MFA) are not supported or applicable by BDXpy.</li> <li>Ensure you have a local BDX username and password for successful authentication that is dedicated to BDXpy. Do not use regular usernames or admin credentials for reoccurring scripts.</li> <li>The credentials used for BDXpy must have BDX security permissions to access the data you are requesting as well as the BDX permissions listed below:<ul> <li>BDXUser</li> <li>TrendViewDataAccess</li> </ul> </li> </ul> <ul> <li>And any device/building/system read permissions necessary found under Manage Path Access if Object Level Security is enabled in BDX.</li> <li>BDXpy is generally intended for use by advanced power users and admin users  even with basic permissions in the BDX app its important to secure your credentials and appoint a BDXpy lead to ensure all uses of the product are being authorized and managed properly.</li> <li>From an authentication perspective BDXpy should be limited in the users BDXpy calls from for audit log and tracking practices. (aka a limited number of users of a single BDX instance and not for mass use by basic security users).</li> </ul>"},{"location":"reference/#component-retrieval-structure","title":"Component Retrieval Structure","text":"<p>BDXpy uses <code>componentPathId</code> to call devices and properties. This applies to buildings and all child devices in BDX. <code>componentPathId</code> is the unique identifier path from the source data agent or parent BDX properties in the hierarchy of devices.</p> <p>Note: <code>componentPathId != componentInstanceID</code></p>"},{"location":"reference/#component-structure-analogy","title":"Component Structure Analogy","text":"<ul> <li>Component path: \"File path\" of a component.</li> <li>Component path ID: \"File inode number.\"</li> <li>This is what BDXpy will use in component retrieval.</li> <li>Component instance ID: File version.</li> </ul>"},{"location":"reference/#defining-devices-for-data-retrieval","title":"Defining Devices for Data Retrieval","text":"<p>Before retrieving data, a user needs to define a set of devices to pull properties from, which will require outlining the <code>componentPathId</code>s of these devices.</p>"},{"location":"reference/#methods-to-lookup-componentpathid","title":"Methods to Lookup <code>componentPathId</code>","text":"<ol> <li>Use methods in BDXpy to lookup or call <code>componentPathId</code>.</li> <li>Manually find the <code>componentPathId</code> under Manage Device Information in BDX.</li> <li>Lookup information in the Custom Query Tool in BDX.</li> </ol>"},{"location":"reference/#overview-of-methods-to-get-componentpathid","title":"Overview of Methods to Get <code>componentPathId</code>","text":"<p>In its simplest form, a <code>componentPathId</code> can be found if you know the <code>componentInstanceId</code>.</p>"},{"location":"reference/#example-retrieve-a-component-by-instance-id","title":"Example: Retrieve a Component by Instance ID","text":"<pre><code>from dotenv import load_dotenv\nimport os\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\n\n# Specify the path to the environment variable file\nload_dotenv(dotenv_path=r\"C:\\Users\\your-path\\.env\")\n\nusername = os.getenv(\"BDX_USERNAME\")\npassword = os.getenv(\"BDX_PASSWORD\")\n\nif not username or not password:\n    raise EnvironmentError(\"Environment variables BDX_USERNAME and BDX_PASSWORD must be set in the .env file\")\n\nauthenticator = UsernameAndPasswordAuthenticator(username, password)\nwith BDX(\"http://yourBDXURL.com\", authenticator) as bdx_instance:\n\n    print(\"Authenticated successfully!\")\n\n    # Retrieve component by ID\n    component_instance_id = 8590005002\n\n    component = bdx_instance.components.by_id(component_instance_id)\n\n    # Extract component details\n    component_name = component.path.displayName\n    full_path = component.path.displayFullPath\n    component_path_id = component.path.componentPathId\n\n    print(f\"Component Name: {component_name}\")\n    print(f\"Component Inst ID: {component_instance_id}\")\n    print(f\"Component Full Path: {full_path}\")\n    print(f\"Component Path ID: {component_path_id}\")\n</code></pre> <p>Output: <pre><code>Authenticated successfully!\nComponent Name: BuildingLogiX Campus\nComponent Inst ID: 4294967534\nComponent Full Path: /Buildings/BuildingLogiX Campus\nComponent Path ID: 4294967534\n</code></pre></p> <p>If you know a building ID you can then print out all its components and their IDs:</p> <p><pre><code>from bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame, AggregationLevel\nimport pandas as pd\nfrom dotenv import load_dotenv\nimport os\n\ndef get_component_table(bdx_instance, building_id):\n    try:\n        # Retrieve components by building ID\n        components = bdx_instance.components.by_building(building_id)\n\n        # Create a list to hold the component details\n        component_data = []\n\n        # Loop through the components and collect the details\n        for component in components:\n            # Extract component details\n            component_name = component.path.displayName\n            full_path = component.path.displayFullPath\n            component_path_id = component.path.componentPathId\n            component_instance_id = component.componentInstanceId  # Add component instance ID\n            entity_type = component.entityTypeName  # Component type\n            template_type = component.templateType  # Template type\n\n            # Append to the component_data list\n            component_data.append({\n                'Component Name': component_name,\n                'Full Path': full_path,\n                'Component Path ID': component_path_id,\n                'Component Instance ID': component_instance_id,  # New column\n                'Entity Type': entity_type,  # Add component type\n                'Template Type': template_type  # Add template type\n            })\n\n        # Convert the list into a Pandas DataFrame\n        df = pd.DataFrame(component_data)\n\n        # Write DataFrame to CSV\n        file_path = r'C:yourPath\\BDXpy\\component_data.csv'\n        df.to_csv(file_path, index=False)\n\n        return df  # Return the DataFrame\n\n    except Exception as e:\n        print(f\"Error retrieving components: {str(e)}\")\n        return None\n\n# Use this function in the run method like this:\ndef run():\n    with BDX(\"https://yourBDXURL.com\", UsernameAndPasswordAuthenticator(\"BDX_USERNAME\", \"BDX_PASSWORD\")) as bdx_instance:\n\n        # Retrieve components by building ID\n        building_id = 4294967302  # Replace with your actual Building ID\n\n        # Get the table of components\n        component_table = get_component_table(bdx_instance, building_id)\n\n        # Display the table\n        if component_table is not None:\n            print(component_table)\n\nif __name__ == \"__main__\":\n    run()\n</code></pre> CSV output: </p>"},{"location":"reference/#example-building-id-retrieval","title":"Example: Building ID Retrieval","text":"<p>To retrieve building data, use the buildings module. The list() method retrieves summaries of all accessible buildings.</p> <p><pre><code>from dotenv import load_dotenv\nimport os\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nimport pandas as pd\n\n# Specify the path to the environment variable file\nload_dotenv(dotenv_path=r\"C:\\Users\\your-path\\.env\")\n\nusername = os.getenv(\"BDX_USERNAME\")\npassword = os.getenv(\"BDX_PASSWORD\")\n\nif not username or not password:\n    raise EnvironmentError(\"Environment variables BDX_USERNAME and BDX_PASSWORD must be set in the .env file\")\n\nauthenticator = UsernameAndPasswordAuthenticator(username, password)\nwith BDX(\"http://yourBDXURL.com\", authenticator) as bdx_instance:\n\n    print(\"Authenticated successfully!\")\n\n###### Retrieve building summaries for all buildings in your BDX site ######\nbuilding_summaries = bdx_instance.buildings.list()\n\n# Convert building summaries to a DataFrame\ndata = [\n    {\n        \"Building Name\": building.name,\n        \"Building ID\": building.componentInstanceId,\n        \"Square Footage\": building.sqft,\n        \"Address\": f\"{building.address.addr1}, {building.address.city}, {building.address.state}\"\n\n    }\n    for building in building_summaries\n]\nbuilding_df = pd.DataFrame(data)\n\n# Display the first few rows of the DataFrame\nprint(building_df)\n</code></pre> Output: </p>"},{"location":"reference/#manage-device-information-in-bdx","title":"Manage Device Information in BDX","text":"<p>Within BDX, users can navigate to Manage Device Information in the upper left navigation. There user can navigate the BDX buildings, hierarchy, and device to look information. This includes the ability to get <code>componentInstanceId</code> which can then be translated with BDXpy to <code>componentPathId</code> per the code below.</p> <p></p> <pre><code># Retrieve component by ID\ncompInstID = 4294967535\ncomponent = bdx.components.by_id(compInstID)\n# Extract component details\ncomponent_name = component.path.displayName\nfull_path = component.path.displayFullPath\ncomponent_path_id = component.path.componentPathId\n\nprint(f\"Component Name: {component_name}\")\nprint(f\"Component Inst ID: {compInstID}\")\nprint(f\"Component Full Path: {full_path}\")\nprint(f\"Component Path ID: {component_path_id}\")\n</code></pre>"},{"location":"reference/#custom-query-tool-lookup-in-bdx","title":"Custom Query Tool lookup in BDX","text":"<p>Navigate to the Custom Query Tool in your BDX instance. In the Advanced Query Mode the various query text below can be pasted and run to retrieve <code>componentPathId</code>.</p>"},{"location":"reference/#building-lookup","title":"Building lookup","text":"<p>This is an example advanced query that can be run from BDX to return values.</p> <p>`select b.name, b.componentInstanceId, b.path.componentPathId  from Building b```</p> <p></p>"},{"location":"reference/#raw-assigned-point-id-lookups","title":"\"Raw\" assigned point ID lookups","text":"<p>The query below looks up any assigned folder or raw points listed under the building (see Manage Buildings &gt; Assigned Point Folders List in BDX). Users can specify/filter the building name in the BDX query below.</p> <p><code>select pf.folderPathAlias,  pf.folderPath.fullPath, pf.folderPath.componentPathId, pf.folderPath.currentComponent.componentInstanceId from Building b  join b.assignedPointFolders pf  where b.name = 'BuildingLogiX Campus'</code></p> <p></p>"},{"location":"reference/#built-in-component-filtering","title":"Built-in Component Filtering","text":"<p><code>ComponentFilter</code> is a filtering mechanism used in BDXpy to refine searches when retrieving building components. It allows filtering by specific attributes like template type, path keyword, and subscription status.</p> <pre><code>from bdx.core import BDX\nfrom bdx.components import ComponentFilter\n</code></pre>"},{"location":"reference/#attributes","title":"Attributes","text":"<ul> <li><code>template_type</code> (str, optional) \u2013 The template type of the component, such as \"VAV\" for Variable Air Volume boxes.</li> <li><code>path_keyword</code> (str, optional) \u2013 A keyword to search for within the component\u2019s path. If filtering by template type alone, this must be set to an empty string (\"\"), otherwise, a 500 error will occur.</li> <li><code>only_subscribed</code> (bool, default False) \u2013 If True, filters only components that are subscribed.</li> </ul>"},{"location":"reference/#usage-examples","title":"Usage Examples","text":"<p>Retrieving VAV Components by Path Keyword To retrieve all VAV components assigned to a building, filtering by a specific keyword in the path:</p> <p><pre><code>filter = ComponentFilter()\nfilter.template_type = \"VAV\"\nfilter.path_keyword = \"AHU_1_VAVs\"\n\nprint(b.components.by_building(4294967304, filter))\n</code></pre> This retrieves all VAV components associated with the building ID 4294967304 that contain \"AHU_1_VAVs\" anywhere in their path.</p>"},{"location":"reference/#filtering-by-template-type-alone","title":"Filtering by Template Type Alone","text":"<p>If filtering by template type only, the path_keyword must be set to an empty string:</p> <pre><code>filter = ComponentFilter()\nfilter.template_type = \"VAV\"\nfilter.path_keyword = \"\"  # Must be explicitly set to avoid error\n\nprint(b.components.by_building(4294967304, filter))\n</code></pre>"},{"location":"reference/#retrieving-only-subscribed-components","title":"Retrieving Only Subscribed Components","text":"<p>To fetch only subscribed components, set only_subscribed to True:</p> <pre><code>filter = ComponentFilter()\nfilter.template_type = \"VAV\"\nfilter.path_keyword = \"AHU_1_VAVs\"\nfilter.only_subscribed = True\n\nprint(b.components.by_building(4294967304, filter))\nThis will return only components that meet the filtering criteria and are subscribed.\nSearching All Buildings for a Filter\nTo can search the entire BDX site for components with a certain name by looping through all buildings and applying filters to the components assigned to each building.\nimport pandas as pd\nfrom bdx.core import BDX\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.components import ComponentFilter\n\n# Authenticate and initialize BDX\nauth = UsernameAndPasswordAuthenticator(\"your_username\", \"your_password\")\nbdx_instance = BDX(\"https://your-bdx-instance.com\", auth)\n\n# Get a list of all buildings\nbuilding_summaries = bdx_instance.buildings.list()\n\n# Create a filter to search for components\nfilter = ComponentFilter()\nfilter.template_type = \"VAV\"\nfilter.path_keyword = \"AHU_1_VAVs\"  # Adjust search term\n\n# Store results in a list\nall_components = []\n\nfor building in building_summaries:\n    building_id = building.componentInstanceId\n    print(f\"Searching in building: {building.name} (ID: {building_id})\")\n\n    # Retrieve components matching the filter in this building\n    components = bdx_instance.components.by_building(building_id, filter)\n\n    # Add retrieved components to the list\n    for comp in components:\n        all_components.append({\n            \"Building Name\": building.name,\n            \"Building ID\": building_id,\n            \"Component ID\": comp.componentInstanceId,\n            \"Component Path ID\": comp.path.componentPathId if comp.path else None,\n            \"Template Type\": comp.templateType,\n            \"Path\": comp.path.fullPath if comp.path else None,\n            \"Display Name\": comp.path.displayName if comp.path else None,\n            \"Data Collection ID\": comp.path.clientDataCollectionId if comp.path else None\n        })\n\n# Convert to DataFrame\ndf = pd.DataFrame(all_components)\n\n# Display the results\nprint(df)\n\n# Save to CSV for analysis (optional)\ndf.to_csv(\"filtered_components_across_buildings.csv\", index=False)\n</code></pre>"},{"location":"reference/#data-retrieval","title":"Data Retrieval","text":"<p>Now that you have components, you can select properties of components and retrieve data. The Trending module and function in BDXpy allows you to retrieve time series data for various properties of components. Once the timeseries data is in a data frame in python various filters, transformations, analysis, etc. can be applied. There are several methods in which to call data. Data can be called right from a Trendview ID or by <code>componentPathId</code>. After retrieving data the names/column labels a user might desire modifications as BDXpy returns the <code>componentPathId</code> in the column header. This can be revised to a custom-user defined label or if calling view Trendview ID inherit the trend labels.</p>"},{"location":"reference/#methods-for-data-retrieval","title":"Methods for Data Retrieval","text":""},{"location":"reference/#method-1-retrieve-data-using-component-pathproperty-combinations","title":"Method 1 - retrieve data using component path/property combinations","text":"<p><pre><code>from bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame\n\ndef main():\nwith BDX(\"http://yourBDXURL.com\", UsernameAndPasswordAuthenticator(\"BDX_USERNAME\", \"BDX_PASSWORD\")) as bdx_instance:\n\n# 21474864282 is an example ID of an AHU. Below calls that AHU and some properties\n        r = bdx_instance.trending.retrieve_data([\n            {\n                \"propertyName\": \"coolOutput\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"ductStaticPressure\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"ductStaticPressureSetpoint\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"heatOutput\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"mixedAirTemp\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"outdoorAirDamperPos\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"outdoorAirTemp\",\n                \"componentPathId\": 21474864282,\n            }])\n\n # Output the first few rows and variables of data        \n        print(r.dataframe)\n\n# Information about the columns can also be printed to help see things like \u2018deviceType\u2019 or \u2018displayName\u2019        \n\n        print(r.column_information)\nmain()\n</code></pre> Outputs: <pre><code>time  21474864282_coolOutput  ...  21474864282_outdoorAirDamperPos  21474864282_outdoorAirTemp\n0   2024-12-27 00:00:00-05:00                     0.0  ...                              0.0                   42.300949\n1   2024-12-27 00:15:00-05:00                     0.0  ...                              0.0                   42.148983\n2   2024-12-27 00:30:00-05:00                     0.0  ...                              0.0                   42.351444\n3   2024-12-27 00:45:00-05:00                     0.0  ...                              0.0                   42.401859\n4   2024-12-27 01:00:00-05:00                     0.0  ...                              0.0                   42.602707\n\n[716 rows x 8 columns]\n{'21474864282_coolOutput': {'propertyName': 'coolOutput', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}, '21474864282_ductStaticPressure': {'propertyName': 'ductStaticPressure', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}, '21474864282_ductStaticPressureSetpoint': {'propertyName': 'ductStaticPressureSetpoint', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}, '21474864282_heatOutput': {'propertyName': 'heatOutput', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}, '21474864282_mixedAirTemp': {'propertyName': 'mixedAirTemp', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}, '21474864282_outdoorAirDamperPos': {'propertyName': 'outdoorAirDamperPos', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}, '21474864282_outdoorAirTemp': {'propertyName': 'outdoorAirTemp', 'componentPathId': 21474864282, 'virtualPath': None, 'lineStyle': 'normal', 'dataTableOnly': False, 'glyphStyle': 'Circle', 'deviceInfo': {'deviceDto': {'id': 470987, 'deviceType': 'ErwDrawThrough', 'nativeDeviceId': 21474836486}, 'displayName': 'AHU1'}}}\n</code></pre></p>"},{"location":"reference/#method-2-retrieve-a-pre-defined-trend-then-get-its-data","title":"Method 2 - retrieve a pre-defined trend, then get its data","text":"<p><pre><code>from bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame\n\ndef main():\nwith BDX(\"http://yourBDXURL.com\", UsernameAndPasswordAuthenticator(\"BDX_USERNAME\", \"BDX_PASSWORD\")) as bdx_instance:\n\n# 635 is the trend ID which can be found in trendview\n\n        t = bdx_instance.trending.trends(635)\n        d = t.retrieve_data(TimeFrame.last_7_days())\n\n        print(d.dataframe)\n\n\nmain()\n</code></pre> Outputs: <pre><code>time  73014470773_realPowerDemand  73014470744_realPowerDemand  fbaaca38439e857f939d395d7948c5510c8d05f7_totalDemand\n0   2024-12-30 11:30:00-05:00                  1180.125122                   944.100098                                        2124.225220\n1   2024-12-30 11:45:00-05:00                  1314.464478                  3286.161133                                        4600.625610\n2   2024-12-30 12:00:00-05:00                  1321.848755                  3304.621826                                        4626.470581\n3   2024-12-30 12:15:00-05:00                  1330.281860                  3325.704590                                        4655.986450\n4   2024-12-30 12:30:00-05:00                  1434.879639                  3874.174805                                        5309.054443\n</code></pre></p>"},{"location":"reference/#finding-trendview-id","title":"Finding Trendview ID","text":"<p>A list of all saved trends can be called in BDXpy. A hyperlink of a saved trend can be referenced like in the image below. </p> <p>Pasted hyperlink with highlighted trend ID 635: <code>http://yourBDXURL.com/trendview/index.html?open=635&amp;timeframe=last7Days&amp;aggregationLevel=Point</code></p>"},{"location":"reference/#names-and-labels","title":"Names and Labels","text":"<p>Now that you have data lets rename the labels by replacing the <code>componentPathId</code> in the column headers with the TrendView label or the equipment display name.</p>"},{"location":"reference/#renaming-labels-in-bdxpy","title":"Renaming Labels in BDXpy","text":""},{"location":"reference/#example-1-displayname-and-property-as-column-labels","title":"Example 1: displayName and Property as column labels","text":"<pre><code>from bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame\n\ndef main():\nwith BDX(\"http://yourBDXURL.com\", UsernameAndPasswordAuthenticator(\"BDX_USERNAME\", \"BDX_PASSWORD\")) as bdx_instance:\n\n# 21474864282 is an example ID of an AHU. Below calls that AHU and some properties\n        r = bdx_instance.trending.retrieve_data([\n            {\n                \"propertyName\": \"coolOutput\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"ductStaticPressure\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"ductStaticPressureSetpoint\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"heatOutput\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"mixedAirTemp\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"outdoorAirDamperPos\",\n                \"componentPathId\": 21474864282,\n            },\n            {\n                \"propertyName\": \"outdoorAirTemp\",\n                \"componentPathId\": 21474864282,\n            }])\n\n# Extract the dataframe from the response\n    df = r.dataframe  # Now 'df' holds the DataFrame with your data\n\n    # 'r.column_information' contains the metadata for the columns\n    column_information = r.column_information\n\n    new_column_names = {}  # Create an empty dictionary to store the new column names.\n\n    # Loop through each column and its associated metadata in the column_information dictionary.\n    for col, info in column_information.items():\n        # Try to get the 'label' from the metadata. If no label is provided, it will return None.\n        label = info.get('label', None)\n\n        if label:\n            # If a 'label' exists, use it as the new column name.\n            new_column_names[col] = label\n        else:\n            # If no 'label' is available, we fall back to constructing a column name\n            # based on the device's display name and the property name.\n            device_name = info['deviceInfo']['displayName']\n            property_name = info['propertyName']\n            new_column_names[col] = f\"{device_name}_{property_name}\"\n\n    # Apply the new column names to the DataFrame.\n    df.rename(columns=new_column_names, inplace=True)\n    print(df.head())\n\nmain()\n</code></pre> <p>Notice how in the outputs the <code>componentPathId</code> is replaced with the <code>displayName</code> of the device and columns are labeled <code>AHU1_ductStaticPressure</code> instead of <code>21474864282 ductStaticPressure</code></p> <p>Output: <pre><code>time  AHU1_coolOutput  AHU1_ductStaticPressure  ...  AHU1_mixedAirTemp  AHU1_outdoorAirDamperPos  AHU1_outdoorAirTemp\n0 2024-12-27 00:00:00-05:00              0.0                 -0.02514  ...          66.657455                       0.0            42.300949\n1 2024-12-27 00:15:00-05:00              0.0                 -0.02514  ...          66.688950                       0.0            42.148983\n2 2024-12-27 00:30:00-05:00              0.0                 -0.02514  ...          66.688950                       0.0            42.351444\n3 2024-12-27 00:45:00-05:00              0.0                 -0.02514  ...          66.657455                       0.0            42.401859\n4 2024-12-27 01:00:00-05:00              0.0                 -0.02514  ...          66.625923                       0.0            42.602707\n</code></pre></p>"},{"location":"reference/#example-2-modifying-labels-from-a-trendview-id-function","title":"Example 2: Modifying Labels from a Trendview ID function","text":"<pre><code>import pandas as pd\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame\n\n# Function to call the BDX API and retrieve data\ndef api_call(trend_id, timeframe):\n    try:\n        with BDX(\"http://yourBDXURL.com\", UsernameAndPasswordAuthenticator(\"BDX_USERNAME\", \"BDX_PASSWORD\")) as bdx_instance:\n            trend = bdx_instance.trending.trends(trend_id)\n            data = trend.retrieve_data(timeframe)\n            df = data.dataframe\n\n            # Use true column names from displayName if available\n            true_column_names = [value['deviceInfo']['displayName'] if 'deviceInfo' in value and 'displayName' in value['deviceInfo'] else key \n                                 for key, value in data.column_information.items()]\n            df.columns = ['time'] + true_column_names[:len(df.columns) - 1] \n            return df\n\n    except Exception as e:\n        print(f\"Error fetching data from API: {e}\")\n        return None\n\ndef main():\n    try:\n        trend_ids = 597\n        timeframe = TimeFrame.last_n_days(4)\n        df = api_call(trend_ids, timeframe)\n\n        if df is None:\n            print(\"No data returned from API.\")\n            return\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\nmain ()\n</code></pre> <p>Output: <pre><code>time  N_Street_Elec  N_Hill_Elec  BuildingLogiX Campus\n0   2024-12-30 11:30:00-05:00    1180.125122   944.100098           2124.225220\n1   2024-12-30 11:45:00-05:00    1314.464478  3286.161133           4600.625610\n2   2024-12-30 12:00:00-05:00    1321.848755  3304.621826           4626.470581\n3   2024-12-30 12:15:00-05:00    1330.281860  3325.704590           4655.986450\n</code></pre></p>"},{"location":"reference/#replace-labels-manually-in-python","title":"Replace Labels Manually in Python","text":"<p>If you manually want to create a list to rename a data frame columns by matching on IDs, something like these functions can be added to your code:</p> <pre><code>def get_custom_display_names(data_response, custom_display_names):\n    \"\"\"\n    Renames DataFrame columns based on custom display names and 'propertyName' from component metadata.\n\n    Parameters:\n    data_response: The response object containing both the DataFrame and column metadata.\n    custom_display_names (dict): A dictionary mapping componentPathId to custom display names.\n\n    Returns:\n    DataFrame: DataFrame with columns renamed as 'CustomDisplayName_propertyName'.\n    \"\"\"\n    # Retrieve the DataFrame and column information\n    df = data_response.dataframe\n    column_information = data_response.column_information\n\n    new_column_names = {}\n\n    # Loop through each column and its metadata\n    for col, info in column_information.items():\n        component_path_id = info.get('componentPathId')\n\n        # Use custom display name if available; otherwise, fall back to displayName\n        display_name = custom_display_names.get(component_path_id, info.get('deviceInfo', {}).get('displayName', 'Unknown'))\n\n        # Ensure 'propertyName' exists\n        property_name = info.get('propertyName', 'UnknownProperty')\n\n        # Construct new column name as 'CustomDisplayName_propertyName'\n        new_column_names[col] = f\"{display_name}_{property_name}\"\n\n    # Rename DataFrame columns\n    df.rename(columns=new_column_names, inplace=True)\n\n    return df\n\ndf_compressors = get_custom_display_names(data_response, custom_display_names)\n</code></pre>"},{"location":"reference/#timeframes-and-aggregations","title":"Timeframes and Aggregations","text":"<p>BDXpy allows users to retrieve trend data for specific timeframes and aggregate the data based on desired intervals. This provides flexibility for analyzing trends over varying periods and resolutions.</p>"},{"location":"reference/#timeframes","title":"Timeframes","text":"<p>Timeframes define the duration of the data to retrieve. BDXpy provides several predefined options, such as:</p> <ul> <li><code>TimeFrame.last_7_days()</code> for data from the past seven days.</li> <li><code>TimeFrame.last_30_days()</code> for data from the past 30 days.</li> <li><code>TimeFrame.last_n_days()</code> for specifying a discrete number of days.</li> </ul> <p>Users can also define custom timeframes by specifying a start and end datetime that is covered in the examples below.</p>"},{"location":"reference/#aggregations","title":"Aggregations","text":"<p>Aggregations control how data is grouped or summarized over time. The default mode is point-level samples and doesn\u2019t need called in the retrieval function. Aggregation options include for example:  </p> <ul> <li><code>AggregationLevel.POINT</code></li> <li><code>AggregationLevel.HOURLY</code></li> <li><code>AggregationLevel.DAILY</code></li> <li><code>AggregationLevel.WEEKLY</code></li> <li><code>AggregationLevel.MONTHLY</code></li> <li><code>AggregationLevel.YEARLY</code></li> </ul> <p>Combining timeframes and aggregation levels allows users to tailor their trend data queries to their specific needs.</p>"},{"location":"reference/#example-retrieve-trend-data-with-timeframes-and-aggregations","title":"Example: Retrieve Trend Data with Timeframes and Aggregations","text":"<pre><code>from bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.core import BDX\nfrom bdx.types import TimeFrame, AggregationLevel\n\n# Replace with your credentials and BDX URL\nBDX_URL = \"http://your-bdx-url.com\"\nUSERNAME = \"your_username\"\nPASSWORD = \"your_password\"\n\nauthenticator = UsernameAndPasswordAuthenticator(USERNAME, PASSWORD)\n\nwith BDX(BDX_URL, authenticator) as bdx_instance:\n    # Retrieve a specific trend\n    trend = bdx_instance.trending.trends(635)\n\n    # Retrieve data for the last 7 days with daily aggregation\n    trend_data1 = trend.retrieve_data(\n        timeframe=TimeFrame.last_7_days(),\n        aggregation_level=AggregationLevel.DAILY\n    )\n    # Retrieve data for the last 60 days with hourly aggregation\n    trend_data2 = trend.retrieve_data(\n        timeframe=TimeFrame.last_n_days(60),\n        aggregation_level=AggregationLevel.HOURLY\n    )\n</code></pre>"},{"location":"reference/#python-exception-errors","title":"Python Exception Errors","text":"<p>The package includes custom exceptions for handling errors that might occur when running a python script with BDXpy:  </p> <ul> <li><code>AuthenticationError</code> \u2192 When calling BDXSession() or BDX() and login credentials fail.</li> <li><code>HttpRequestError</code> \u2192 When retrieving trends, buildings, or components from the BDX API.</li> <li><code>DataNotFoundError</code> \u2192 When requesting a component, trend, or time series that doesn\u2019t exist.</li> <li><code>SecurityError</code> \u2192 When trying to access a restricted resource (e.g., a building component you don\u2019t have access to).</li> </ul>"},{"location":"reference/#bdxpy-modules-user-facing-only","title":"BDXpy Modules (User-Facing Only)","text":"<ol> <li><code>auth.py</code> \u2013 Manages authentication, including username/password login.</li> <li><code>buildings.py</code> \u2013 Allows users to retrieve building data, including addresses, zones, and assigned components.</li> <li><code>components.py</code> \u2013 Enables component lookups, including retrieving information about HVAC systems, meters, sensors, and other building components.</li> <li><code>core.py</code> \u2013 Provides the main interface for interacting with BDX, wrapping session management and data retrieval.</li> <li><code>trending.py</code> \u2013 Handles trend data retrieval, allowing users to query, filter, and analyze time-series data from BDX.</li> <li><code>types.py</code> \u2013 Defines helper classes, enumerations, and error handling for API interactions.</li> </ol>"},{"location":"examples/comingsoon/","title":"COMING SOON....","text":""},{"location":"examples/Automated%20Reporting/automatedReporting/","title":"Automated Reporting Examples","text":"<p>This guide walks you through setting up BDXpy as a FastAPI data service on a Grafana server and configuring it as a JSON API data source.</p>"},{"location":"examples/Automated%20Reporting/automatedReporting/#example-pdf-generation-of-monthly-energy-report","title":"Example PDF generation of monthly energy report","text":"<p>Below is example code where you can insert BDXpy code to generate a PDF from a combination of images and tables. In the second example these principles can be applied to create a PDF that gets emailed.</p> Show Code <pre><code>    import numpy as np\n    import pandas as pd\n    import plotly.express as px\n    import plotly.graph_objects as go\n    import imgkit\n    import time\n    import plotly.io as pio\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from pandas.plotting import table\n    from selenium import webdriver\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from reportlab.lib.pagesizes import letter\n    from reportlab.lib import colors\n    from reportlab.lib.styles import getSampleStyleSheet\n    from reportlab.lib.units import inch\n    from reportlab.lib.utils import ImageReader\n    from reportlab.pdfgen import canvas\n    from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph\n    from datetime import datetime\n    import os\n    from reportlab.lib.pagesizes import letter\n    from reportlab.platypus import SimpleDocTemplate, Image, Paragraph\n    from reportlab.lib.styles import getSampleStyleSheet\n    from reportlab.lib.units import inch\n    from PIL import Image as PILImage\n\n    print(\"Current Working Directory:\", os.getcwd())\n\n    # Function to save the DataFrame to a PNG\n    def save_dataframe_to_png(df, file_name):\n        # Set up the plot\n        fig, ax = plt.subplots(figsize=(12, 6))  # Adjust size as necessary\n        ax.axis('off')  # No axes\n        ax.axis('tight')\n\n        # Create the table from the DataFrame\n        table_data = table(ax, df, loc='center', cellLoc='center', colWidths=[0.1] * len(df.columns))\n\n        # Save the figure as a PNG\n        plt.savefig(file_name, bbox_inches='tight', dpi=300)\n\n\n    # Assuming 'fig' is your Plotly figure object\n    def save_plotly_as_png(fig, output_filename):\n        # Save the figure as a PNG file\n        pio.write_image(fig, output_filename)\n\n    def save_html_to_file(fig, file_name):\n        # Save the Plotly figure as an HTML file\n        fig.write_html(file_name)\n\n    config = imgkit.config(wkhtmltoimage=r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltoimage.exe')\n\n    def html_to_png(html_file, output_image):\n        time.sleep(2)  # Adjust the delay as necessary\n        imgkit.from_file(html_file, output_image, config=config)\n\n    #### BDXpy function would go here along with component selections #####\n\n    df=data\n\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n\n    # Ensure your start and end dates are in datetime format\n    start_date = pd.to_datetime('2024-09-01')\n    end_date = pd.to_datetime('2024-09-30')\n\n    # Filter the DataFrame\n    filtered_df = df[(df['Timestamp'] &gt;= start_date) &amp; (df['Timestamp'] &lt;= end_date)]\n    filtered_df.to_csv(\"filtered_df.csv\", index=False)\n\n    # Define columns to keep without aggregation (taking the first occurrence)\n    cols_to_keep = [\"Building Name\", \"Building Type\", \"Latitude\", \"Longitude\"]\n\n    # Define columns to sum\n    cols_to_sum = [\"Total Energy\", \"Electric Energy\", \"Cooling Energy\", \"Heating Energy\",\n                \"Total Cost\", \"Metric Tons CO2\"]\n\n    # Group by 'Building Name', summing only the relevant columns\n    df_summed = filtered_df.groupby(\"Building Name\", as_index=False).agg(\n        {**{col: 'first' for col in cols_to_keep},  # Keep first occurrence\n        **{col: 'sum' for col in cols_to_sum},  # Sum numeric values\n        \"Timestamp\": \"count\"}  # Count total rows\n    )\n\n    # Rename the counted column\n    df_summed = df_summed.rename(columns={\"Timestamp\": \"Total Rows Summed\"})\n    df_summed.to_csv(\"df_summed.csv\", index=False)\n\n    # Create a map with bubble size variation and opacity\n    fig = px.scatter_mapbox(\n        df_summed,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Building Name',\n        size='Total Energy',  # Bubble size based on Total Energy\n        color='Building Type',  # Color based on Building Type\n        size_max=40,  # Max bubble size\n        zoom=14,\n        mapbox_style=\"carto-positron\",\n        text='Building Name', \n        opacity = 0.6,\n        title= \"Energy Consumption by Building Type\"\n    )\n\n    # Set map layout\n    fig.update_layout(\n        mapbox_center={\"lat\": 51.7540, \"lon\": -1.2577},\n        margin={\"r\": 10, \"t\": 100, \"l\": 10, \"b\": 10},\n        height=800  # Adjust height for better layout\n    )\n\n    fig.update_traces(marker=dict(opacity=0.6))\n\n    fig.show()\n    print(fig.data[0])\n\n\n    # Example usage\n    fig.write_html(\"mapenergychart.html\")\n    # Save the figure as a PNG file\n    fig.write_image(\"mapenergychart.png\", format=\"png\", scale=2)\n\n    df_expanded = df\n\n    # Ensure 'Timestamp' column is in datetime format if not already\n    df_expanded['Timestamp'] = pd.to_datetime(df_expanded['Timestamp'])\n\n    # Extract the year from the 'Timestamp' column\n    df_expanded['Year'] = df_expanded['Timestamp'].dt.year\n\n    # Group by year and building type, calculating the sum of total cost\n    cost_summary = df_expanded.groupby(['Building Type', 'Year'])['Total Cost'].sum().unstack().fillna(0)\n\n    # Round cost values to the nearest whole number\n    cost_summary = cost_summary.round(0)\n\n    # Calculate percent change for the latest year compared to the previous year\n    cost_summary['Percent Change'] = cost_summary.pct_change(axis=1).iloc[:, -1] * 100\n\n    # Calculate actual cost change (difference between current year and previous year)\n    last_year = cost_summary.columns[-2]  # Second to last year (previous year)\n    current_year = cost_summary.columns[-3]  # Last year column (current year)\n\n    cost_summary['Actual Cost Change'] = cost_summary[last_year] - cost_summary[current_year]\n\n    # Add a totals row at the bottom of the DataFrame\n    totals_row = cost_summary.sum(numeric_only=True)\n    totals_row.name = 'Total'\n    cost_summary = pd.concat([cost_summary, totals_row.to_frame().T], axis=0)\n\n    # Styling the DataFrame for a report\n    def color_percent(series):\n        \"\"\"Color green for negative (decrease), red for positive (increase).\"\"\"\n        return ['color: green' if val &lt; 0 else 'color: red' if val &gt; 0 else 'color: black' for val in series]\n\n    # Apply formatting to all year columns dynamically\n    year_columns = [col for col in cost_summary.columns if isinstance(col, int)]  # Select only year columns\n\n    # Apply formatting and coloring\n    styled_summary = cost_summary.style.format({\n        **{year: '${:,.0f}' for year in year_columns},  # Apply currency format to all year columns\n        'Percent Change': '{:.2f}%',                    # Format percent change\n        'Actual Cost Change': '${:,.0f}'                 # Format actual cost change as currency\n    }).apply(color_percent, subset=['Percent Change', 'Actual Cost Change'])\n\n    # Display or save to HTML for review\n    styled_summary.to_html('cost_summary_report.html')\n\n    # Show the styled DataFrame\n    styled_summary\n\n    # Use the function to save your cost_summary DataFrame\n    # save_dataframe_to_png(cost_summary, 'cost_summary.png')\n\n    # Replicating the logic for BTU for Total Energy instead of cost, rounding to the nearest whole number\n\n    df_expanded = df_expanded.copy()  # Assuming df_expanded is your main DataFrame\n\n    # Ensure 'Timestamp' column is in datetime format if not already\n    df_expanded['Timestamp'] = pd.to_datetime(df_expanded['Timestamp'])\n\n    # Extract the year from the 'Timestamp' column\n    df_expanded['Year'] = df_expanded['Timestamp'].dt.year\n\n    # Group by year and building type, calculating the sum of total energy (BTU)\n    energy_summary = df_expanded.groupby(['Building Type', 'Year'])['Total Energy'].sum().unstack().fillna(0)\n\n    # Round energy values to the nearest whole number\n    energy_summary = energy_summary.round(0)\n\n    # Calculate percent change for the latest year compared to the previous year\n    energy_summary['Percent Change'] = energy_summary.pct_change(axis=1).iloc[:, -1] * 100\n\n    # Calculate actual energy change (difference between current year and previous year)\n    last_year = energy_summary.columns[-2]  # Second to last year (previous year)\n    current_year = energy_summary.columns[-3]  # Last year column (current year)\n\n    energy_summary['Actual Energy Change'] = energy_summary[last_year] - energy_summary[current_year]\n\n    # Add a totals row at the bottom of the DataFrame\n    totals_row = energy_summary.sum(numeric_only=True)\n    totals_row.name = 'Total'\n    energy_summary = pd.concat([energy_summary, totals_row.to_frame().T], axis=0)\n\n    # Styling the DataFrame for a report\n    def color_percent(series):\n        \"\"\"Color green for negative (decrease), red for positive (increase).\"\"\"\n        return ['color: green' if val &lt; 0 else 'color: red' if val &gt; 0 else 'color: black' for val in series]\n\n    # Apply formatting to all year columns dynamically\n    year_columns = [col for col in energy_summary.columns if isinstance(col, int)]  # Select only year columns\n\n    # Apply formatting and coloring\n    styled_energy_summary = energy_summary.style.format({\n        **{year: '{:,.0f}' for year in year_columns},  # Apply whole number format to all year columns\n        'Percent Change': '{:.2f}%',                    # Format percent change\n        'Actual Energy Change': '{:,.0f}'                 # Format actual energy change\n    }).apply(color_percent, subset=['Percent Change', 'Actual Energy Change'])\n\n    # Display or save to HTML for review\n    styled_energy_summary.to_html('energy_summary_report.html')\n\n    # Show the styled DataFrame\n    styled_energy_summary\n\n    # Use the function to save your cost_summary DataFrame\n    # save_dataframe_to_png(energy_summary, 'energy_summary.png')\n\n\n\n    # Assuming there is a 'Metric Tons CO2' column or we generate one based on energy (Total Energy * emission factor)\n    # You can change the factor to match your data.\n    emission_factor = 0.0001  # Example: 0.0001 Metric Tons of CO2 per BTU of energy\n\n    # Create Metric Tons CO2 column if it doesn't exist\n    if 'Metric Tons CO2' not in df_expanded.columns:\n        df_expanded['Metric Tons CO2'] = df_expanded['Total Energy'] * emission_factor\n\n    # Extract the year from the 'Timestamp' column\n    df_expanded['Year'] = df_expanded['Timestamp'].dt.year\n\n    # Group by year and building type, calculating the sum of Metric Tons CO2\n    co2_summary = df_expanded.groupby(['Building Type', 'Year'])['Metric Tons CO2'].sum().unstack().fillna(0)\n\n    # Round CO2 values to the nearest whole number\n    co2_summary = co2_summary.round(0)\n\n    # Calculate percent change for the latest year compared to the previous year\n    co2_summary['Percent Change'] = co2_summary.pct_change(axis=1).iloc[:, -1] * 100\n\n    # Calculate actual CO2 change (difference between current year and previous year)\n    last_year = co2_summary.columns[-2]  # Second to last year\n    current_year = co2_summary.columns[-3]  # Last year column\n\n    co2_summary['Actual CO2 Change'] = co2_summary[last_year] - co2_summary[current_year]\n\n    # Add a totals row at the bottom of the DataFrame\n    totals_row = co2_summary.sum(numeric_only=True)\n    totals_row.name = 'Total'\n    co2_summary = pd.concat([co2_summary, totals_row.to_frame().T], axis=0)\n\n    # Styling the DataFrame for a report\n    def color_percent(series):\n        \"\"\"Color green for negative (decrease), red for positive (increase).\"\"\"\n        return ['color: green' if val &lt; 0 else 'color: red' if val &gt; 0 else 'color: black' for val in series]\n\n    # Apply formatting to all year columns dynamically\n    year_columns = [col for col in co2_summary.columns if isinstance(col, int)]  # Select only year columns\n\n    # Apply formatting and coloring\n    styled_co2_summary = co2_summary.style.format({\n        **{year: '{:,.0f}' for year in year_columns},  # Apply whole number format to all year columns\n        'Percent Change': '{:.2f}%',                   # Format percent change\n        'Actual CO2 Change': '{:,.0f}'                 # Format actual CO2 change\n    }).apply(color_percent, subset=['Percent Change', 'Actual CO2 Change'])\n\n    # Display or save to HTML for review\n    styled_co2_summary.to_html('co2_summary_report.html')\n\n    # Show the styled DataFrame\n    styled_co2_summary\n\n    # Use the function to save your cost_summary DataFrame\n    # save_dataframe_to_png(co2_summary, 'co2_summary.png')\n\n\n    # Group the data by year and month, summing the total energy for each month of each year\n    df_expanded['Month'] = df_expanded['Timestamp'].dt.month\n    monthly_energy_by_year = df_expanded.groupby(['Year', 'Month'])['Total Energy'].sum().unstack(level=0).fillna(0)\n\n    # Convert the DataFrame to long format for Plotly\n    monthly_energy_long = monthly_energy_by_year.reset_index().melt(id_vars='Month', var_name='Year', value_name='Total Energy')\n\n    # Create a Plotly chart\n    fig = px.bar(monthly_energy_long, x='Month', y='Total Energy', color='Year', barmode='group', \n                labels={'Total Energy': 'Total Energy (BTU)', 'Month': 'Month'},\n                title='Monthly Total Energy Consumption by Year')\n\n    # Show the chart\n    fig.show()\n\n\n    # Example usage\n    fig.write_html(\"yearoveryearchart.html\")\n    fig.write_image(\"yearoveryearchart.png\", format=\"png\", scale=2)\n\n    # Group the data by building and year to calculate the total energy for each building\n    building_energy_2023 = df_expanded[df_expanded['Year'] == 2023].groupby('Building Name')[['Electric Energy', 'Heating Energy']].sum()\n    building_energy_2024 = df_expanded[df_expanded['Year'] == 2024].groupby('Building Name')[['Electric Energy', 'Heating Energy']].sum()\n\n    # Compute percent changes between 2023 and 2024\n    building_percent_change = pd.DataFrame()\n    building_percent_change['Percent Change Electric'] = ((building_energy_2024['Electric Energy'] - building_energy_2023['Electric Energy']) / building_energy_2023['Electric Energy']) * 100\n    building_percent_change['Percent Change Heating'] = ((building_energy_2024['Heating Energy'] - building_energy_2023['Heating Energy']) / building_energy_2023['Heating Energy']) * 100\n    building_percent_change['Total Energy'] = building_energy_2024.sum(axis=1)  # Total energy in 2024\n    building_percent_change['Building Type'] = df_expanded.groupby('Building Name')['Building Type'].first()  # Get building type\n    building_percent_change = building_percent_change.reset_index()\n\n    # Create the updated bubble chart\n    fig = px.scatter(\n        building_percent_change, \n        x='Percent Change Heating', \n        y='Percent Change Electric',\n        size='Total Energy',\n        color='Building Type',\n        text='Building Name',  # Show building names as visible text labels\n        title='Energy Consumption Changes by University Buildings (2023 vs 2024)',\n        labels={'Percent Change Heating': 'Heating Consumption Change (%)', 'Percent Change Electric': 'Electric Consumption Change (%)'},\n        size_max=60,  # Maximum size for the bubbles\n        template='plotly_white'  # Apply the Plotly white theme\n    )\n\n    # Add vertical and horizontal lines at x=0 and y=0\n    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"black\")\n    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"black\")\n\n    # Customize text color based on conditions and add traces\n    for i, row in building_percent_change.iterrows():\n        text_color = None  # Reset the text_color for each row\n        if row['Percent Change Heating'] &lt; 0 and row['Percent Change Electric'] &lt; 0:\n            text_color = 'green'\n        elif row['Percent Change Heating'] &gt; 0 and row['Percent Change Electric'] &gt; 0:\n            text_color = 'red'\n        else:\n            text_color = 'black'  # Default color for other conditions\n\n        # Add a new trace with the correct text color\n        new_trace = px.scatter(\n            pd.DataFrame([row]), \n            x='Percent Change Heating', \n            y='Percent Change Electric',\n            size='Total Energy',\n            text='Building Name',\n            size_max=60\n        ).data[0]\n\n        # Update the text color for this trace and remove markers\n        new_trace.textfont = dict(color=text_color, size=14)\n        new_trace.marker = dict(size=0, opacity=0)  # Remove the marker but keep the text\n\n        # Add the trace to the figure\n        fig.add_trace(new_trace)\n\n    # Update layout without explicitly specifying font family to use Plotly's default (Open Sans)\n    fig.update_layout(\n        font_size=14,\n        xaxis_title=\"Heating Consumption Change (%)\",\n        yaxis_title=\"Electric Consumption Change (%)\",\n        title_font_size=20,\n        height=800 \n    )\n\n    # Show the updated figure\n    fig.show()\n\n    # Example usage\n    fig.write_html(\"pctchangechart.html\")\n    fig.write_image(\"pctchangechart.png\", format=\"png\", scale=2)\n\n\n\n    # Function to add images with preserved aspect ratio\n    def add_image_with_aspect_ratio(image_path, max_width=6*inch, max_height=4*inch):\n        img = PILImage.open(image_path)\n        width, height = img.size\n        aspect_ratio = width / height\n\n        if width &gt; height:\n            new_width = min(max_width, width * (max_height / height))\n            new_height = new_width / aspect_ratio\n        else:\n            new_height = min(max_height, height * (max_width / width))\n            new_width = new_height * aspect_ratio\n\n        return Image(image_path, width=new_width, height=new_height)\n\n    def create_pdf(output_filename, map_chart, cost_summary_img, energy_summary_img, co2_summary_img, yoy_chart, pct_change_chart):\n        pdf = SimpleDocTemplate(output_filename, pagesize=letter)\n        elements = []\n\n        # Title and summary text\n        styles = getSampleStyleSheet()\n        title = Paragraph(\"Energy Consumption Summary\", styles['Title'])\n        summary_text = Paragraph(\n            \"This report provides an overview of the energy consumption for the month.\", styles['Normal']\n        )\n\n        elements.append(title)\n        elements.append(summary_text)\n\n        # Add map chart image with aspect ratio preserved\n        map_image = add_image_with_aspect_ratio(map_chart)\n        elements.append(map_image)\n\n        # Add year-over-year chart with aspect ratio preserved\n        yoy_image = add_image_with_aspect_ratio(yoy_chart)\n        elements.append(yoy_image)\n\n        # Add percentage change chart with aspect ratio preserved\n        pct_change_image = add_image_with_aspect_ratio(pct_change_chart)\n        elements.append(pct_change_image)\n\n        # Build PDF\n        pdf.build(elements)\n\n    # Call the create_pdf function with PNG paths\n    create_pdf(\"energy_summary.pdf\", \"mapenergychart.png\", \"cost_summary.png\", \"energy_summary.png\", \"co2_summary.png\", \"yearoveryearchart.png\", \"pctchangechart.png\")\n</code></pre>"},{"location":"examples/Automated%20Reporting/automatedReporting/#email-pdf-report","title":"Email PDF Report","text":"<p>A SMTP email service can be used to email this PDF. The python script can then be referenced in a BAT file and scheduled to run from the Windows Task Scheduler on a user specificed interval.</p> Show Code <pre><code>    from bdx.auth import UsernameAndPasswordAuthenticator\n    from bdx.core import BDX\n    from bdx.types import TimeFrame\n    from datetime import datetime, timedelta\n    import os\n    from dotenv import load_dotenv\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from io import BytesIO\n    from reportlab.lib.colors import HexColor\n    from reportlab.lib import colors\n    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, Image\n    from reportlab.lib.pagesizes import A4\n    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle\n    import smtplib\n    from email.message import EmailMessage\n\n    # Load environment variables from .env file\n    load_dotenv()\n\n    SMTP_SERVER = os.getenv(\"SMTP_SERVER\")\n    SMTP_PORT = int(os.getenv(\"SMTP_PORT\"))\n    SMTP_USERNAME = os.getenv(\"SMTP_USERNAME\")\n    SMTP_PASSWORD = os.getenv(\"SMTP_PASSWORD\")\n    RECEIVER_EMAIL = os.getenv(\"RECEIVER_EMAIL\")\n    ALERT_EMAIL = os.getenv(\"ALERT_EMAIL\")\n\n    BDX_USERNAME = os.getenv(\"BDX_USERNAME\")\n    BDX_PASSWORD = os.getenv(\"BDX_PASSWORD\")\n\n    def get_previous_month_dates():\n        today = datetime.now()\n        first_day_of_current_month = today.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n        last_day_of_previous_month = first_day_of_current_month - timedelta(seconds=1)\n        first_day_of_previous_month = last_day_of_previous_month.replace(day=1, hour=0, minute=0, second=0, microsecond=0)\n        return first_day_of_previous_month, last_day_of_previous_month\n\n    def create_pdf_report(data_summary, data_trend):\n        try:\n            pdf_path = \"output/automated_report.pdf\"\n            pdf = SimpleDocTemplate(pdf_path, pagesize=A4)\n            elements = []\n\n            styles = getSampleStyleSheet()\n            title = Paragraph(\"Automated Report Example\", styles['Title'])\n            elements.append(title)\n\n            start_date, end_date = get_previous_month_dates()\n            date_text = Paragraph(f\"Reporting Period: {start_date.strftime('%B %d, %Y')} to {end_date.strftime('%B %d, %Y')}\", styles['BodyText'])\n            elements.append(date_text)\n            elements.append(Spacer(1, 0.2 * A4[1]))\n\n            # Example chart\n            fig, ax = plt.subplots(figsize=(6, 4))\n            ax.plot(data_trend['Date'], data_trend['Value'], marker='o')\n            plt.title(\"Example Data Trend\")\n            plt.xlabel(\"Date\")\n            plt.ylabel(\"Value\")\n            plt.grid(True)\n\n            chart_image = BytesIO()\n            plt.savefig(chart_image, format=\"png\")\n            plt.close(fig)\n            chart_image.seek(0)\n            chart = Image(chart_image, width=5 * A4[0] / 8, height=3 * A4[1] / 8)\n            elements.append(chart)\n\n            # Example data table\n            table_data = [data_summary.columns.to_list()] + data_summary.values.tolist()\n            table = Table(table_data)\n            table.setStyle(TableStyle([\n                ('BACKGROUND', (0, 0), (-1, 0), colors.grey),\n                ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),\n                ('ALIGN', (0, 0), (-1, -1), 'CENTER'),\n                ('GRID', (0, 0), (-1, -1), 1, colors.black)\n            ]))\n            elements.append(table)\n            pdf.build(elements)\n\n            print(f\"PDF report saved to {pdf_path}\")\n            return pdf_path\n        except Exception as e:\n            print(f\"Error generating PDF: {e}\")\n            send_alert_email(f\"Failed to generate PDF report: {e}\")\n            return None\n\n    def send_email_with_pdf(pdf_path):\n        subject = \"Automated Report Example\"\n        body = \"Please find the attached example automated report.\"\n\n        msg = EmailMessage()\n        msg[\"From\"] = SMTP_USERNAME\n        msg[\"To\"] = RECEIVER_EMAIL\n        msg[\"Subject\"] = subject\n        msg.set_content(body)\n\n        with open(pdf_path, \"rb\") as f:\n            msg.add_attachment(f.read(), maintype=\"application\", subtype=\"pdf\", filename=os.path.basename(pdf_path))\n\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n            smtp.starttls()\n            smtp.login(SMTP_USERNAME, SMTP_PASSWORD)\n            smtp.send_message(msg)\n        print(f\"Email with PDF {pdf_path} sent.\")\n\n    def send_alert_email(error_message):\n        subject = \"PDF Generation Alert\"\n        msg = EmailMessage()\n        msg[\"From\"] = SMTP_USERNAME \n        msg[\"To\"] = ALERT_EMAIL \n        msg[\"Subject\"] = subject\n        msg.set_content(f\"An error occurred: {error_message}\")\n\n        with smtplib.SMTP(SMTP_SERVER, SMTP_PORT) as smtp:\n            smtp.starttls()\n            smtp.login(SMTP_USERNAME, SMTP_PASSWORD)\n            smtp.send_message(msg)\n        print(\"Alert email sent due to PDF generation failure.\")\n\n    def main():\n        with BDX(\"https://yourBDXURL.com\", UsernameAndPasswordAuthenticator(BDX_USERNAME, BDX_PASSWORD)) as b:\n            start_date, end_date = get_previous_month_dates()\n            print(f\"Start Date: {start_date}, End Date: {end_date}\")\n\n            # Example dummy data retrieval\n            dummy_data = b.trending.retrieveData([\n                {\"propertyName\": \"value\", \"componentPathId\": 1234567890},\n                {\"propertyName\": \"value\", \"componentPathId\": 1234567891}\n            ], timeframe=TimeFrame(start=start_date, end=end_date))\n\n            df_dummy = pd.DataFrame({\n                \"Date\": pd.date_range(start=start_date, periods=10, freq='D'),\n                \"Value\": [x for x in range(10)]\n            })\n\n            df_summary = pd.DataFrame({\n                \"Metric\": [\"Total Energy\", \"Peak Demand\", \"Average Usage\"],\n                \"Value\": [10000, 500, 350]\n            })\n\n            pdf_path = create_pdf_report(df_summary, df_dummy)\n            if pdf_path:\n                send_email_with_pdf(pdf_path)\n            else:\n                print(\"PDF generation failed. Email not sent.\")\n\n    main()\n</code></pre>"},{"location":"examples/Dashboards/rtu_dash_ex/","title":"RTU Monitor","text":""},{"location":"examples/Dashboards/rtu_dash_ex/#example-rtu-monitoring","title":"Example: RTU Monitoring","text":"<p>Dash app containing standard colors and timeframes for various means of display on RTU data from BDX. This example was deployed on https://render.com/ Beyond the code below there was additional server, css, and enviromental files required to deploy.</p> \ud83d\udd0d Click for Fullscreen <p>This example demonstrates a dashboard for monitoring RTU (Rooftop Unit) performance using BDXpy. It features:</p> <ul> <li>Real-time Data Visualization: Tracks key RTU metrics over time.</li> <li>Interactive Graphs &amp; Charts: Provides insights into energy consumption and efficiency.</li> <li>Responsive Web Design: Accessible on both desktop and mobile.</li> </ul> <p>To explore the dashboard, visit the live demo above or check out the source code.</p> Show Code <pre><code>    import pandas as pd\n    from bdx.core import BDX\n    from bdx.auth import UsernameAndPasswordAuthenticator\n    from bdx.types import TimeFrame, AggregationLevel\n    from bdx.components import ComponentFilter\n    import plotly.express as px\n    import dash\n    from dash import dcc, html, Output, Input, ctx\n    from dash.dependencies import Input, Output\n    import re\n\n    ##### APP Contains commented our BDXpy api sections and loads csv with dummy data instead.\n\n    # BDX Credentials and Connection\n    # BDX_URL = \"http://your_BDX_URL.com\"  # Replace with your actual BDX URL\n    # USERNAME = \"BDX_USERNAME\"\n    # PASSWORD = \"BDX_PASSWORD\"\n    # BUILDING_NAME = \"BuildingLogiX Campus\" #name of the building in bdx for searching equipment ids\n\n    # # Authenticate\n    # auth = UsernameAndPasswordAuthenticator(USERNAME, PASSWORD)\n\n    # with BDX(BDX_URL, auth) as bdx:\n    #     buildings = bdx.buildings.list()\n    #     matching_buildings = [b for b in buildings if b.name.lower() == BUILDING_NAME.lower()]\n    #     if not matching_buildings:\n    #         print(f\"No building found with the name: {BUILDING_NAME}\")\n    #         exit()\n\n    #     BUILDING_ID = matching_buildings[0].componentInstanceId\n    #     BUILDING_NODE = f\"Building: {BUILDING_NAME}\"\n\n    #     print(f\"\\nSelected Building ID: {BUILDING_ID} for '{BUILDING_NAME}'\")\n\n    #     components = bdx.components.by_building(BUILDING_ID)\n\n    #     # Extract relevant component details into a DataFrame\n    #     component_data = []\n\n    #     # Iterate through retrieved components\n    #     for component in components:\n    #         component_data.append({\n    #             \"Component ID\": component.componentInstanceId,\n    #             \"Display Name\": component.path.displayName if component.path else \"N/A\",\n    #             \"Full Path\": component.path.fullPath if component.path else \"N/A\",\n    #             \"Template Type\": component.templateType,\n    #             \"Parent Name\": component.parentName,\n    #             \"Root Parent Name\": component.rootParentName\n    #         })\n\n    #     # Convert to DataFrame\n    #     components_df = pd.DataFrame(component_data)\n\n\n    #     filtered_components = [\n    #         c for c in components\n    #         if c.templateType == \"LargeRtu\" and c.path and \"RTU_\" in c.path.displayName\n    #     ]\n\n    #     # Convert filtered components to a DataFrame\n    #     filtered_data = [\n    #         {\n    #             \"Component Inst ID\": c.componentInstanceId,\n    #             \"Display Name\": c.path.displayName,\n    #             \"Full Path\": c.path.fullPath,\n    #             \"Template Type\": c.templateType,\n    #         }\n    #         for c in filtered_components\n    #     ]\n\n    #     filtered_df = pd.DataFrame(filtered_data)\n\n    #     print(filtered_df)\n\n    #     # Define properties to retrieve\n    #     properties_to_fetch = [\n    #         \"ductStaticPressure\",\n    #         \"ductStaticPressureSetpoint\",\n    #         \"supplyAirTemp\",\n    #         \"supplyFanVFDPercent\",\n    #         \"supplyFanVFDPower\",\n    #         \"coolOutput\",\n    #         \"supplyFanStatus\"\n    #     ]\n\n    #     data_requests = []\n\n    #     # Create property descriptors\n    #     for component in filtered_components:\n    #         if component.path and component.path.componentPathId:  # Ensure the path exists\n    #             for prop in properties_to_fetch:\n    #                 data_requests.append({\n    #                     \"componentPathId\": component.path.componentPathId,\n    #                     \"propertyName\": prop\n    #                 })\n\n    #     # Debugging: Check if requests were created\n    #     print(f\"Total Data Requests: {len(data_requests)}\")\n    #     if len(data_requests) == 0:\n    #         print(\"\u26a0 No data requests were generated! Check component paths.\")\n\n    #     # Fetch trending data for the last 7 days\n    #     timeframe = TimeFrame.last_7_days()\n    #     retrieval_result = bdx.trending.retrieve_data(data_requests, timeframe, AggregationLevel.POINT)\n\n    #     # Convert the data to a Pandas DataFrame\n    #     df = retrieval_result.dataframe\n\n    #     # Create a mapping for renaming columns\n    #     column_renaming = {}\n\n    #     # Iterate through the filtered components to find correct display names\n    #     for component in filtered_components:\n    #         if component.path and component.path.componentPathId:  # Ensure valid path\n    #             for prop in properties_to_fetch:\n    #                 old_column_name = f\"{component.path.componentPathId}_{prop}\"\n    #                 new_column_name = f\"{component.path.displayName}_{prop}\"\n    #                 column_renaming[old_column_name] = new_column_name\n\n    #     # Rename the columns\n    #     df.rename(columns=column_renaming, inplace=True)\n\n    #     # Print sample data with updated column names\n    #     print(df.head())\n\n    # -------------------------------------------------------------------------\n    # 1) Load CSV data for dummy app data\n    # -------------------------------------------------------------------------\n    csv_file = \"bdx_large_rtu_data_ex.csv\"  # Update if needed\n    df = pd.read_csv(csv_file)\n    df['time'] = pd.to_datetime(df['time'])  # Ensure 'time' is a datetime\n\n    # -------------------------------------------------------------------------\n    # 2) Extract RTU names using regex\n    # -------------------------------------------------------------------------\n    rtu_names = sorted(\n        set(re.findall(r'RTU_\\d+', col)[0] for col in df.columns if re.findall(r'RTU_\\d+', col))\n    )\n\n    # -------------------------------------------------------------------------\n    # 3) Define a color map for RTUs\n    # -------------------------------------------------------------------------\n    color_seq = px.colors.qualitative.Dark24  # or choose another\n    color_map = {}\n    for i, rtu in enumerate(rtu_names):\n        color_map[rtu] = color_seq[i % len(color_seq)]\n\n    # For wide-format line plots, define a color sequence matching rtu_names order\n    wide_format_colors = [color_map[rtu] for rtu in rtu_names]\n\n    # -------------------------------------------------------------------------\n    # 4) Define GPS coordinates for the 17 RTUs (example data)\n    # -------------------------------------------------------------------------\n    rtu_gps_data = {\n        \"RTU#\": [f\"RTU_{i}\" for i in range(1, 18)],\n        \"Latitude\": [\n            52.00837569274067, 52.00830627568983, 52.00875609626744, 52.00854506989283,\n            52.009003217993765, 52.00886993902999, 52.009108730225236, 52.009291987720204,\n            52.0095002339626, 52.009336413666574, 52.009850085476714, 52.0101804982761,\n            52.0103470920397, 52.00682072496339, 52.006956786798256, 52.006543046465836,\n            52.0071900346995\n        ],\n        \"Longitude\": [\n            -0.6927407350645639, -0.6933361764343267, -0.6924385034602146, -0.6916310488754607,\n            -0.6915904506002495, -0.6919964333523606, -0.6904852753306141, -0.690088314417439,\n            -0.6904762534916784, -0.6932865563201799, -0.6924610580520515, -0.6922445339175922,\n            -0.6918746385212245, -0.6932053597447845, -0.6927768223953339, -0.6926550275697007,\n            -0.6916265378920792\n        ]\n    }\n    rtu_gps_df = pd.DataFrame(rtu_gps_data)\n\n    # Calculate average lat/lon for map centering\n    center_lat = rtu_gps_df[\"Latitude\"].mean()\n    center_lon = rtu_gps_df[\"Longitude\"].mean()\n\n    # -------------------------------------------------------------------------\n    # 5) Dash App layout\n    # -------------------------------------------------------------------------\n    app = dash.Dash(__name__)\n\n    # Define server (needed for deployment)\n    server = app.server \n\n    app.layout = html.Div([\n        html.Div([\n            html.Img(src=\"/assets/blx white.svg\", className=\"logo\"),  # Logo (Make sure it's in the \"assets\" folder)\n            html.H1(\"BuildingLogiX RTU Monitoring\", className=\"header-text\")\n        ], className=\"header\", style={\"backgroundColor\": \"#00274D\", \"padding\": \"15px\", \"textAlign\": \"center\"}),\n        html.Div([\n            # Left column\n            html.Div([\n                # Top row: Map\n                dcc.Graph(id='map-runtime', style={'height': '700px'}, config={'scrollZoom': True}),\n\n                # Middle row: Violin (SupplyAirTemp, fan on)\n                dcc.Graph(id='violin-supply-air'),\n\n                # Bottom row: Time series (SupplyAirTemp)\n                dcc.Graph(id='time-series-supply-air')\n            ], style={'width': '50%', 'display': 'inline-block'}),\n\n            # Right column\n            html.Div([\n                # Top row: Polar (VFD Power)\n                dcc.Graph(id='polar-vfd-power', style={'height': '700px'}),\n\n                # Middle row: Violin (Duct Static, fan on)\n                dcc.Graph(id='kde-duct-static'),\n\n                # Bottom row: Time series (SupplyFanSpeed)\n                dcc.Graph(id='time-series-duct-static')\n            ], style={'width': '50%', 'display': 'inline-block'})\n        ])\n    ])\n\n    # -------------------------------------------------------------------------\n    # 6) Dash Callback for all figures\n    # -------------------------------------------------------------------------\n    @app.callback(\n        Output('map-runtime', 'figure'),\n        Output('violin-supply-air', 'figure'),\n        Output('time-series-supply-air', 'figure'),\n        Output('polar-vfd-power', 'figure'),\n        Output('kde-duct-static', 'figure'),\n        Output('time-series-duct-static', 'figure'),\n        Input('map-runtime', 'clickData'),\n    )\n\n    def update_charts(_):\n        \"\"\"\n        Returns six figures for the dashboard:\n        1) Map showing RTU runtime hours (marker size), labeled with RTU name + hours\n        2) Violin plot of Supply Air Temp (fan on)\n        3) Time series of Supply Air Temp (wide format)\n        4) Polar chart of VFD Power (most recent sample)\n        5) Violin plot of Duct Static Pressure (fan on)\n        6) Time series of Supply Fan Speed (wide format)\n        \"\"\"\n\n        # ---------------------------------------------------------------------\n        # A) Most recent data row (for polar chart)\n        # ---------------------------------------------------------------------\n        latest_data = df.iloc[-1]\n\n        # ---------------------------------------------------------------------\n        # B) Map: RTU Runtime Hours\n        # ---------------------------------------------------------------------\n        # Summation of supplyFanStatus over all rows =&gt; total \"on\" intervals\n        # Each row is presumably 15 minutes =&gt; multiply by 0.25 to convert to hours\n        runtime_values = [\n            df[f\"{rtu}_supplyFanStatus\"].sum() * 0.25\n            for rtu in rtu_names\n        ]\n\n        # Build a DF for the map\n        runtime_df = rtu_gps_df.copy()\n        runtime_df[\"RuntimeHours\"] = runtime_values\n\n        # Add a label with RTU name and hours on separate lines\n        runtime_df[\"Label\"] = runtime_df[\"RTU#\"] + \"&lt;br&gt;\" + runtime_df[\"RuntimeHours\"].round(1).astype(str) + \" hrs\"\n\n\n        map_fig = px.scatter_mapbox(\n            runtime_df,\n            lat=\"Latitude\",\n            lon=\"Longitude\",\n            size=\"RuntimeHours\",\n            size_max=20,\n            color=\"RuntimeHours\",  # Assigns a color gradient based on runtime\n            color_continuous_scale=\"Viridis\",  # You can use \"Plasma\", \"Cividis\", \"Turbo\", etc.\n            hover_name=\"RTU#\",      # Shown in hover tooltip\n            text=\"Label\",           # Shown on the map\n            title=\"RTU Runtime Map - Last 7 Days\",\n            zoom=15.5,\n            center=dict(lat=center_lat, lon=center_lon),\n            mapbox_style=\"carto-positron\"\n        )\n\n        # Display the labels above each marker\n        map_fig.update_traces(\n            mode=\"markers+text\",\n            textposition=\"bottom center\"\n        )\n\n        map_fig.update_layout(\n            dragmode=\"pan\",\n            uirevision=\"constant\",\n            mapbox=dict(\n                pitch=60,  # Tilt for 3D effect\n                bearing=0,  # Rotate the view\n                style=\"carto-positron\"\n            )\n        )\n\n\n\n        # ---------------------------------------------------------------------\n        # C) Violin: SupplyAirTemp (fan on)\n        # ---------------------------------------------------------------------\n        sat_dfs = []\n        for rtu in rtu_names:\n            sub_df = df[['time', f\"{rtu}_supplyAirTemp\", f\"{rtu}_supplyFanStatus\"]].copy()\n            sub_df.rename(columns={\n                f\"{rtu}_supplyAirTemp\": \"SupplyAirTemp\",\n                f\"{rtu}_supplyFanStatus\": \"SupplyFanStatus\"\n            }, inplace=True)\n            sub_df[\"RTU\"] = rtu\n            sat_dfs.append(sub_df)\n\n        melted_sat = pd.concat(sat_dfs, ignore_index=True)\n\n        # Filter to fan on (True or ==1, depending on your data)\n        melted_sat = melted_sat[melted_sat[\"SupplyFanStatus\"] == True]\n        melted_sat.dropna(subset=[\"SupplyAirTemp\"], inplace=True)\n\n        # Sort RTUs by descending average supply air temp\n        avg_sat = melted_sat.groupby(\"RTU\")[\"SupplyAirTemp\"].mean().sort_values(ascending=False)\n        rtu_order_sat = list(avg_sat.index)\n\n        violin_sat_fig = px.violin(\n            melted_sat,\n            x=\"RTU\",\n            y=\"SupplyAirTemp\",\n            color=\"RTU\",\n            color_discrete_map=color_map,\n            category_orders={\"RTU\": rtu_order_sat},\n            box=True,\n            points=False,\n            hover_data=[\"time\"],\n            title=\"Operating Supply Air Temp\"\n        )\n\n        # ---------------------------------------------------------------------\n        # D) Time Series: SupplyAirTemp (wide format)\n        # ---------------------------------------------------------------------\n        supply_temp_fig = px.line(\n            df,\n            x=\"time\",\n            y=[f\"{rtu}_supplyAirTemp\" for rtu in rtu_names],\n            title=\"Supply Air Temp Over Time\",\n            color_discrete_sequence=wide_format_colors\n        )\n\n        # ---------------------------------------------------------------------\n        # E) Polar Chart: Supply Fan VFD Power (most recent)\n        # ---------------------------------------------------------------------\n        polar_df = pd.DataFrame({\n            \"RTU\": rtu_names,\n            \"VFDPower\": [latest_data[f\"{rtu}_supplyFanVFDPower\"] for rtu in rtu_names]\n        })\n\n        polar_fig = px.bar_polar(\n            polar_df,\n            r=\"VFDPower\",\n            theta=\"RTU\",\n            color=\"RTU\",\n            color_discrete_map=color_map,\n            title=\"RTU kW (Most Recent)\"\n        )\n\n        # ---------------------------------------------------------------------\n        # F) Violin: Duct Static Pressure (fan on)\n        # ---------------------------------------------------------------------\n        rtu_dfs_2 = []\n        for rtu in rtu_names:\n            sub_df = df[['time', f\"{rtu}_ductStaticPressure\", f\"{rtu}_supplyFanStatus\"]].copy()\n            sub_df.rename(columns={\n                f\"{rtu}_ductStaticPressure\": \"DuctStaticPressure\",\n                f\"{rtu}_supplyFanStatus\": \"SupplyFanStatus\"\n            }, inplace=True)\n            sub_df[\"RTU\"] = rtu\n            rtu_dfs_2.append(sub_df)\n\n        melted_static = pd.concat(rtu_dfs_2, ignore_index=True)\n\n        # Filter to fan on (True or 1)\n        melted_static = melted_static[melted_static[\"SupplyFanStatus\"] == True]\n        melted_static.dropna(subset=[\"DuctStaticPressure\"], inplace=True)\n\n        avg_static = melted_static.groupby(\"RTU\")[\"DuctStaticPressure\"].mean().sort_values(ascending=False)\n        rtu_order = list(avg_static.index)\n\n        kde_fig = px.violin(\n            melted_static,\n            x=\"RTU\",\n            y=\"DuctStaticPressure\",\n            color=\"RTU\",\n            color_discrete_map=color_map,\n            category_orders={\"RTU\": rtu_order},\n            box=True,\n            points=False,\n            hover_data=[\"time\"],\n            title=\"Operating Duct Static Pressure\"\n        )\n\n        # ---------------------------------------------------------------------\n        # G) Time Series: Supply Fan Speed (wide format)\n        # ---------------------------------------------------------------------\n        speed_fig = px.line(\n            df,\n            x=\"time\",\n            y=[f\"{rtu}_supplyFanVFDPercent\" for rtu in rtu_names],\n            title=\"Supply Fan Speed Over Time\",\n            color_discrete_sequence=wide_format_colors\n        )\n\n        return (\n            map_fig,\n            violin_sat_fig,\n            supply_temp_fig,\n            polar_fig,\n            kde_fig,\n            speed_fig\n        )\n\n\n\n    if __name__ == '__main__':\n        app.run_server(debug=False)\n</code></pre>"},{"location":"examples/Data%20Service/grafanaDataService/","title":"Setting Up BDXpy as a Data Service for Grafana","text":"<p>This guide walks you through setting up BDXpy as a FastAPI data service on a Grafana server and configuring it as a JSON API data source.</p> <p></p>"},{"location":"examples/Data%20Service/grafanaDataService/#1-install-dependencies","title":"1. Install Dependencies","text":"<p>Ensure you have Python 3.12+ installed. Install the necessary dependencies:     <pre><code>    pip install fastapi uvicorn pandas requests python-dotenv\n</code></pre></p>"},{"location":"examples/Data%20Service/grafanaDataService/#2-set-up-environment-variables","title":"2. Set Up Environment Variables","text":"<p>Create a .env file to store your BDX credentials securely:     <pre><code>    BDX_URL=\"https://your-bdx-instance.com\"\n    BDX_USER=\"your-username\"\n    BDX_PASS=\"your-password\"\n</code></pre> Note: your server the data service is being run on will need access to reach the bdx URL Because BDXpy handles authentication on additional authentication is needed in grafana.</p>"},{"location":"examples/Data%20Service/grafanaDataService/#3-implement-the-fastapi-service","title":"3. Implement the FastAPI Service","text":"<p>Create a file bdx_service.py and define the API:</p> <pre><code>```python\n    from fastapi import FastAPI, Query, HTTPException, BackgroundTasks, Request\n    from fastapi.responses import JSONResponse \n    import pandas as pd\n    import uvicorn\n    import os\n    import logging\n    import json\n    from datetime import datetime, timedelta, timezone\n    from dotenv import load_dotenv\n    from slowapi import Limiter\n    from slowapi.util import get_remote_address\n    from typing import List, Optional\n    from memory_profiler import profile\n\n    # Import BDXpy modules\n    from bdx.core import BDX\n    from bdx.auth import UsernameAndPasswordAuthenticator\n    from bdx.trending import PropertyDescriptor\n    from bdx.types import TimeFrame, AggregationLevel\n\n    # Configure Logging\n    logger = logging.getLogger(__name__)\n    log_handler = logging.handlers.RotatingFileHandler(\"api.log\", maxBytes=100*1024*1024, backupCount=5)\n    logger.setLevel(logging.DEBUG)\n    log_handler.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\"))\n    logger.addHandler(log_handler)\n\n    def log_event(level, message, extra=None):\n        log_entry = {\n            \"level\": level, \n            \"message\": message, \n            \"timestamp\": datetime.now(timezone.utc).isoformat()\n        }\n        if extra:\n            try:\n                json.dumps(extra)  # Ensure extra is JSON serializable\n                log_entry[\"extra\"] = extra\n            except TypeError:\n                log_entry[\"extra\"] = str(extra)  # Convert non-serializable objects to strings\n        logger.log(level, json.dumps(log_entry))\n\n    # Load environment variables\n    load_dotenv()\n    BDX_URL = os.getenv(\"BDX_URL\")\n    BDX_USER = os.getenv(\"BDX_USER\")\n    BDX_PASS = os.getenv(\"BDX_PASS\")\n\n    # Initialize Rate Limiting\n    limiter = Limiter(key_func=get_remote_address)\n\n    # FastAPI app\n    app = FastAPI()\n\n    # Initialize BDX connection\n    authenticator = UsernameAndPasswordAuthenticator(BDX_USER, BDX_PASS)\n    bdx_instance = BDX(BDX_URL, authenticator)\n\n    @app.middleware(\"http\")\n    async def error_middleware(request: Request, call_next):\n        try:\n            return await call_next(request)\n        except Exception as e:\n            log_event(logging.ERROR, f\"Unhandled error: {e}\", {\"request_path\": request.url.path})\n            return JSONResponse(\n                status_code=500,\n                content={\"message\": \"Internal Server Error\"}\n            )\n\n    @app.get(\"/query\")\n    @limiter.limit(\"20/minute\")\n    async def query_grafana(\n        request: Request,\n        background_tasks: BackgroundTasks,\n        component_path_ids: str = Query(..., title=\"Comma-separated list of Component Path IDs\"),  \n        properties: str = Query(\"value\", title=\"Comma-separated list of properties\"),  \n        start_time: Optional[str] = Query(None, title=\"Start Time (ISO 8601, default: 24h ago)\"),\n        end_time: Optional[str] = Query(None, title=\"End Time (ISO 8601, default: now)\")\n    ):\n        log_event(logging.DEBUG, \"Raw query parameters\", {\"query_params\": dict(request.query_params)})\n\n        try:\n            component_path_ids_list = list(map(int, component_path_ids.split(\",\")))\n            properties_list = properties.split(\",\")\n\n            if not start_time:\n                start_time = (datetime.now(timezone.utc) - timedelta(hours=24)).isoformat()\n            if not end_time:\n                end_time = datetime.now(timezone.utc).isoformat()\n\n            log_event(logging.DEBUG, \"Parsed query parameters\", {\n                \"component_path_ids\": component_path_ids_list,\n                \"properties\": properties_list,\n                \"start_time\": start_time,\n                \"end_time\": end_time\n            })\n\n            response = await get_bdxpy_trend_data(bdx_instance, component_path_ids_list, properties_list, start_time, end_time)\n            return response\n        except Exception as e:\n            log_event(logging.ERROR, f\"Unexpected error in query processing: {str(e)}\")\n            raise HTTPException(status_code=500, detail=\"Internal server error\")\n\n\n    async def get_bdxpy_trend_data(bdx_instance, component_path_ids: List[int], properties: List[str], start_time: str, end_time: str):\n        try:\n            log_event(logging.DEBUG, \"Fetching BDXpy data\", {\n                \"component_path_ids\": component_path_ids,\n                \"properties\": properties,\n                \"start_time\": start_time,\n                \"end_time\": end_time\n            })\n\n            trending = bdx_instance.trending\n            start_dt = datetime.fromisoformat(start_time).astimezone(timezone.utc)\n            end_dt = datetime.fromisoformat(end_time).astimezone(timezone.utc)\n            timeframe = TimeFrame(start=start_dt, end=end_dt)\n\n            property_descriptors = [\n                PropertyDescriptor(componentPathId=comp_id, propertyName=prop)\n                for comp_id in component_path_ids\n                for prop in properties\n            ]\n\n            trend_data = trending.retrieve_data(properties=property_descriptors, timeframe=timeframe, aggregation_level=AggregationLevel.POINT)\n            df = trend_data.dataframe\n\n            log_event(logging.DEBUG, \"Received DataFrame\", {\"df_shape\": df.shape, \"df_columns\": list(df.columns)})\n\n            if df.empty:\n                log_event(logging.WARNING, \"No data returned from BDXpy.\")\n                return {\"message\": \"No data available\"}\n\n            if 'time' in df.columns:\n                df['time'] = pd.to_datetime(df['time'], utc=True)  # Ensure 'time' column is datetime\n                df.set_index('time', inplace=True)  # Set 'time' as the index\n\n            print(df)\n\n            response = [\n                {\n                    \"target\": column.split('_')[-1],  # Remove component ID prefix to match Grafana target names\n                    \"datapoints\": [\n                        [value, int(pd.Timestamp(timestamp).timestamp() * 1000)]\n                        for timestamp, value in df[column].items() if pd.notna(timestamp) and not pd.isnull(value)\n                    ]\n                }\n                for column in df.columns if column != 'time'\n            ]\n            return response\n        except Exception as e:\n            log_event(logging.ERROR, f\"Error fetching BDXpy data: {e}\")\n            raise HTTPException(status_code=500, detail=\"Internal Server Error\")\n\n    @app.get(\"/\")\n    async def root():\n        return {\"message\": \"BDXpy API for Grafana\"}\n\n    if __name__ == \"__main__\":\n        uvicorn.run(app, host=\"127.0.0.1\", port=8000)\n```\n</code></pre>"},{"location":"examples/Data%20Service/grafanaDataService/#4-run-the-api-service","title":"4. Run the API Service","text":"<p>Start the service:     <code>python           python bdx_service.py</code> Your FastAPI service is now running on port 8000. Note: if this is a important process you will want to create additional monitoring and error checking and maybe expand service restart scenarios to ensure high uptime</p>"},{"location":"examples/Data%20Service/grafanaDataService/#5-configure-grafana","title":"5. Configure Grafana","text":"<ul> <li> <p>Install JSON API</p> </li> <li> <p>Add a New Data Source in Grafana:</p> </li> <li>Navigate to Configuration \u2192 Data Sources \u2192 Add Data Source.</li> <li>Select \"JSON API\".</li> <li>Set:<ul> <li>URL: <code>http://localhost:8000</code></li> <li>Method: <code>GET</code></li> </ul> </li> <li> <p>Click Save &amp; Test.</p> </li> <li> <p>Create a Panel:</p> </li> <li>Go to Dashboards \u2192 Create \u2192 New Panel.</li> <li>Choose the JSON API data source.</li> <li>Set the Query Path to <code>/query</code>.</li> <li> <p>Add Query Parameters:</p> <ul> <li><code>component_path_ids=21474864282</code></li> <li><code>properties=supplyAirTemp,ductStaticPressure</code></li> <li><code>start_time=${__from:date}</code></li> <li><code>end_time=${__to:date}</code></li> </ul> </li> <li> <p>Format Response in Grafana:</p> </li> <li>Use JSONPath Queries to extract data:     <pre><code>[\n  {\n    \"name\": \"value\",\n    \"jsonPath\": \"$[*].datapoints[*][0]\",\n    \"type\": \"number\"\n  },\n  {\n    \"name\": \"timestamp\",\n    \"jsonPath\": \"$[*].datapoints[*][1]\",\n    \"type\": \"time\"\n  },\n  {\n    \"name\": \"metric\",\n    \"jsonPath\": \"$[*].target\",\n    \"type\": \"string\"\n  }\n]\n</code></pre></li> </ul>"},{"location":"examples/Data%20Service/grafanaDataService/#6-verify-data-in-grafana","title":"6. Verify Data in Grafana","text":"<ul> <li>Ensure your panel correctly visualizes each series separately.</li> <li>If a boolean point (e.g., <code>supplyFanStatus</code>) is coming back null, check:</li> <li>The property name in BDX.</li> <li>The data type (it may need conversion to <code>1/0</code> instead of <code>true/false</code>).</li> </ul>"},{"location":"examples/Data%20Service/grafanaDataService/#7-optional-enable-secondary-y-axis-in-grafana","title":"7. (Optional) Enable Secondary Y-Axis in Grafana","text":"<p>To plot multiple metrics with different scales: - Go to Panel Settings \u2192 Axes. - Under Y-Axis, set:   - Y-Axis 1 for <code>supplyAirTemp</code>   - Y-Axis 2 for <code>ductStaticPressure</code></p>"},{"location":"examples/Data%20Service/grafanaDataService/#8-optional-add-background-shading-for-boolean-points","title":"8. (Optional) Add Background Shading for Boolean Points","text":"<p>To visualize a Boolean (<code>True/False</code>) metric: - Use Field Overrides:   - Set a Threshold (e.g., <code>1 = ON</code> / <code>0 = OFF</code>).   - Change background color dynamically based on status.</p>"},{"location":"examples/Data%20Visualization/VAV_network/","title":"Building CFM Network","text":"<p>This example demonstrates the relationships, location, and changes related to airflow across different air systems within a building</p>"},{"location":"examples/Data%20Visualization/VAV_network/#output","title":"Output","text":"\ud83d\udd0d Click for Fullscreen Show Code <pre><code>import networkx as nx\nfrom pyvis.network import Network\nimport pandas as pd\nfrom bdx.core import BDX\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.types import TimeFrame, AggregationLevel\nfrom bdx.components import ComponentFilter\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\n\n# BDX Credentials and Connection\nBDX_URL = \"http://yourURL.com\"  # Replace with your actual BDX URL\nUSERNAME = \"YOUR_USERNAME\"\nPASSWORD = \"YOUR_PASSWORD\"\nBUILDING_NAME = \"YOUR_BUILDINGNAME\"\n\n# Connect to BDX\nauth = UsernameAndPasswordAuthenticator(USERNAME, PASSWORD)\nwith BDX(BDX_URL, auth) as bdx:\n    buildings = bdx.buildings.list()\n    matching_buildings = [b for b in buildings if b.name.lower() == BUILDING_NAME.lower()]\n    if not matching_buildings:\n        print(f\"No building found with the name: {BUILDING_NAME}\")\n        exit()\n\n    BUILDING_ID = matching_buildings[0].componentInstanceId\n    BUILDING_NODE = f\"Building: {BUILDING_NAME}\"\n\n    print(f\"\\nSelected Building ID: {BUILDING_ID} for '{BUILDING_NAME}'\")\n\n    # Retrieve all components\n    all_components = bdx.components.by_building(building_id=BUILDING_ID)\n\n    # Define AHUs of interest\n    AHU_NUMBERS = [1, 2, 3, 4, 6, 8]\n    ahu_names = {f\"AHU_{num}\": f\"AHU {num}\" for num in AHU_NUMBERS}\n\n    # Prepare dict to hold AHU -&gt; list of VAV components\n    ahu_components = {ahu: [] for ahu in ahu_names}\n\n    # Manually filter for VAVs that belong to these AHUs\n    vav_components = [\n        comp for comp in all_components \n        if \"VAV_\" in comp.path.displayName \n        and any(comp.path.displayName.startswith(f\"VAV_{ahu}_\") for ahu in AHU_NUMBERS)\n    ]\n\n    # Map each VAV displayName -&gt; AHU_x\n    vav_to_ahu = {}\n    for vav in vav_components:\n        ahu_number = vav.path.displayName.split(\"_\")[1]  # Extract \"1\" from \"VAV_1_xxx\"\n        if f\"AHU_{ahu_number}\" in ahu_names:\n            vav_to_ahu[vav.path.displayName] = f\"AHU_{ahu_number}\"\n            ahu_components[f\"AHU_{ahu_number}\"].append(vav)\n\n    # Retrieve airFlow data for two timeframes\n    timeframe_current = TimeFrame.last_7_days()\n    timeframe_previous = TimeFrame.last_n_days(14)  # last 14 days, but we'll only compare the first 7 to the last 7\n\n    properties = [{\"componentPathId\": vav.path.componentPathId, \"propertyName\": \"airFlow\"} for vav in vav_components]\n\n    # Get current week data (7 days)\n    trend_data_current = bdx.trending.retrieve_data(properties, timeframe_current, AggregationLevel.HOURLY)\n    df_current = trend_data_current.dataframe.fillna(0).set_index(\"time\")\n\n    # Get previous week data (7 days before that)\n    trend_data_previous = bdx.trending.retrieve_data(properties, timeframe_previous, AggregationLevel.HOURLY)\n    df_previous = trend_data_previous.dataframe.fillna(0).set_index(\"time\")\n    # Trim previous to same length as current (assumes same # of hours)\n    df_previous = df_previous.iloc[: len(df_current)]\n\n    # Aggregate total airflow for both timeframes\n    total_airflow_current = df_current.sum().to_dict()   # e.g. {'compId_airFlow': totalCFM, ...}\n    total_airflow_previous = df_previous.sum().to_dict()\n\n# -----------------------------------------------------------------------------\n# 1) Compute all percent differences in one pass\n# -----------------------------------------------------------------------------\nall_percent_diffs = {}\nall_current_airflows = {}\n\nfor vav_comp in vav_components:\n    comp_id = vav_comp.path.componentPathId\n    current_airflow = total_airflow_current.get(f\"{comp_id}_airFlow\", 0)\n    previous_airflow = total_airflow_previous.get(f\"{comp_id}_airFlow\", 0)\n    if previous_airflow != 0:\n        percent_diff = ((current_airflow - previous_airflow) / previous_airflow) * 100\n    else:\n        percent_diff = 0\n\n    # Store these results by VAV displayName\n    all_percent_diffs[vav_comp.path.displayName] = percent_diff\n    all_current_airflows[vav_comp.path.displayName] = current_airflow\n\n# If everything is empty, avoid errors\nif len(all_percent_diffs) == 0:\n    vmin, vmax = -1, 1\nelse:\n    vmin = min(all_percent_diffs.values())\n    vmax = max(all_percent_diffs.values())\n\n# -----------------------------------------------------------------------------\n# 2) Create the TwoSlopeNorm and colormap once\n# -----------------------------------------------------------------------------\ncolormap = plt.get_cmap(\"RdBu_r\")  # Blue for negative, red for positive\nnorm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n\n# For consistent node sizing, get the max current airflow across all VAVs\nif len(all_current_airflows) == 0:\n    overall_max_airflow = 1\nelse:\n    overall_max_airflow = max(all_current_airflows.values())\n\n# -----------------------------------------------------------------------------\n# 3) Build the PyVis network\n# -----------------------------------------------------------------------------\nnet = Network(height=\"1000px\", width=\"100%\", notebook=True, directed=False)\n\n# Enable physics for dynamic spacing (avoids overlap)\nnet.barnes_hut(gravity=-7000, central_gravity=0.2, spring_length=50, spring_strength=0.03)\n\n# Add building node\nnet.add_node(\n    BUILDING_NODE, \n    size=100, \n    color=\"#3e3e3e\", \n    label=f\"Building: Apex Pavilion\", \n    physics=True,\n    font={\"size\": 50}\n)\n\n# Add AHU nodes &amp; connect them to the building\nfor ahu, ahu_label in ahu_names.items():\n    net.add_node(\n        ahu, \n        size=50, \n        color=\"#f5d76e\", \n        label=f\"P1_{ahu_label}\", \n        physics=True,\n        font={\"size\": 40}\n    )\n    net.add_edge(BUILDING_NODE, ahu)\n\n    # Get the VAV displayNames belonging to this AHU\n    ahu_vavs = [vav for vav, linked_ahu in vav_to_ahu.items() if linked_ahu == ahu]\n\n    # Sort them by absolute airflow change if you wish\n    # (We can derive airflow change from the stored percent or from the actual flows if needed.)\n    # For demonstration, let's just get the current minus previous from all_percent_diffs if needed.\n    # But you already had a dictionary \"airflow_differences\" if you want to re-use it.\n    # We'll do a quick inline approach:\n    def get_airflow_change(vav_disp_name):\n        # We can reconstruct from percent_diffs or better, do a direct sum again:\n        # But let's use the dictionary \"airflow_differences\" from your original code.\n        # If you still want that, we can compute it similarly:\n        comp = next((c for c in vav_components if c.path.displayName == vav_disp_name), None)\n        if comp is None:\n            return 0\n        comp_id = comp.path.componentPathId\n        cur_val = total_airflow_current.get(f\"{comp_id}_airFlow\", 0)\n        prev_val = total_airflow_previous.get(f\"{comp_id}_airFlow\", 0)\n        return (cur_val - prev_val)\n\n    sorted_vavs = sorted(\n        ahu_vavs,\n        key=lambda name: abs(get_airflow_change(name)),\n        reverse=True\n    )\n\n    # Loop through each VAV\n    for vav_disp_name in sorted_vavs:\n        # Grab the current airflow &amp; percent diff we stored\n        current_airflow = all_current_airflows.get(vav_disp_name, 0)\n        percent_diff = all_percent_diffs.get(vav_disp_name, 0)\n\n        # Convert the percent_diff to a color\n        rgba_color = colormap(norm(percent_diff))  # e.g. negative =&gt; bluish, positive =&gt; reddish\n        hex_color = mcolors.to_hex(rgba_color)\n\n        # Scale VAV size by current airflow\n        node_size = 20 + (50 * (current_airflow / max(1, overall_max_airflow)))\n\n        # Add VAV node\n        net.add_node(\n            vav_disp_name, \n            size=node_size, \n            color=hex_color,\n            title=f\"{vav_disp_name} - % Change: {percent_diff:.2f}%, Airflow: {current_airflow:.2f}\",\n            physics=True,\n            font={\"size\": 30}\n        )\n\n        # Add edge from AHU -&gt; VAV\n        net.add_edge(\n            ahu,\n            vav_disp_name,\n            width=1,\n            title=f\"% Change: {percent_diff:.2f}%\"\n        )\n\n# Generate and save the network visualization\nnet.show(\"VAV_network.html\")\n\nprint(\"\\n\u2705 Network visualization saved as 'VAV_network.html'. Open it in a browser to view.\")\n</code></pre>"},{"location":"examples/Data%20Visualization/ahuEcono_ex/","title":"AHU Economizer Analysis","text":"<p>This example demonstrates a detailed review of AHU economizing conditions</p>"},{"location":"examples/Data%20Visualization/ahuEcono_ex/#output","title":"Output","text":"Show Code <pre><code>import pandas as pd\nimport plotly.graph_objects as go\nimport numpy as np\n\n# # Load CSV data\n# df = pd\n\n\n# Dynamically identify the relevant column names\noccupancy_col = [col for col in df.columns if 'occupancystatus' in col.lower()][0]\noat_col = [col for col in df.columns if 'outdoorairtemp' in col.lower()][0]\nmat_col = [col for col in df.columns if 'mixedairtemp' in col.lower()][0]\nrat_col = [col for col in df.columns if 'returnairtemp' in col.lower()][0]\n\n# Filter data for rows where occupancyStatus is 'occupied' and drop rows with NaN in key columns\noccupied_data = df[df[occupancy_col].str.lower() == 'occupied']\noccupied_data = occupied_data.dropna(subset=[oat_col, mat_col, rat_col])\n\n# Convert columns to numeric\noccupied_data[oat_col] = pd.to_numeric(occupied_data[oat_col], errors='coerce')\noccupied_data[mat_col] = pd.to_numeric(occupied_data[mat_col], errors='coerce')\noccupied_data[rat_col] = pd.to_numeric(occupied_data[rat_col], errors='coerce')\n\n# Drop rows with any remaining NaN after conversion\noccupied_data = occupied_data.dropna(subset=[oat_col, mat_col, rat_col])\n\n# Extract min and max values for axes\noat_min = occupied_data[oat_col].min()\noat_max = occupied_data[oat_col].max()\n\n# Calculate thresholds\nsupply_air_setpoint = 55\nred_threshold = supply_air_setpoint - 1  # 1 degree below supply air setpoint\naverage_rat = occupied_data[rat_col].mean()  # Average return air temperature\ngreen_threshold = average_rat + 3  # 3 degrees higher than average RAT\n\n# Define the piecewise function for the ideal line\ndef ideal_line(oat):\n    if oat &lt;= 53:\n        return 53\n    elif oat &lt;= 70:\n        return 53 + (oat - 53) * 1\n    else:\n        return 70\n\n# Generate data points for the ideal line\noat_range = np.linspace(oat_min, green_threshold, 500)  # Extend range to green threshold\nmat_ideal = [ideal_line(oat) for oat in oat_range]\n\n# Control bands around the ideal line\nupper_band = [mat + 2 for mat in mat_ideal]\nlower_band = [mat - 2 for mat in mat_ideal]\n\n# Define the percentage of outside air for minimum operation\npercent_outside_air = 0.2  # 20% outside air\n\n# Calculate the dynamic minimum outside air line\nmin_outside_air = oat_range * percent_outside_air + average_rat * (1 - percent_outside_air)\n\n# Control bands around the dynamic minimum outside air line\nmin_outside_air_upper_band = min_outside_air + 2\nmin_outside_air_lower_band = min_outside_air - 2\n\n# Create the plot using Plotly\nfig = go.Figure()\n\n# Scatter plot of observed data\nfig.add_trace(go.Scatter(\n    x=occupied_data[oat_col],\n    y=occupied_data[mat_col],\n    mode='markers',\n    marker=dict(color='black', size=6, opacity=0.6),\n    name='Observed Data'\n))\n\n# Ideal line and control bands\nfig.add_trace(go.Scatter(\n    x=oat_range,\n    y=mat_ideal,\n    mode='lines',\n    line=dict(color='darkblue', width=3),\n    name='Ideal Economizing Line'\n))\nfig.add_trace(go.Scatter(\n    x=oat_range,\n    y=upper_band,\n    mode='lines',\n    line=dict(color='darkblue', dash='dot', width=2),\n    name='Upper Control Band'\n))\nfig.add_trace(go.Scatter(\n    x=oat_range,\n    y=lower_band,\n    mode='lines',\n    line=dict(color='darkblue', dash='dot', width=2),\n    name='Lower Control Band'\n))\n\n# Vertical lines with dynamic thresholds\nfig.add_vline(\n    x=red_threshold,\n    line_color='red',\n    line_dash='dash',\n    line_width=2,\n    annotation_text=f'Red Threshold ({red_threshold}\u00b0F)',\n    annotation_position='top left'\n)\nfig.add_vline(\n    x=green_threshold,\n    line_color='green',\n    line_dash='dash',\n    line_width=2,\n    annotation_text=f'Green Threshold ({green_threshold:.2f}\u00b0F)',\n    annotation_position='top left'\n)\n\n# Diagonal 100% outside air line (MAT = OAT)\nfig.add_trace(go.Scatter(\n    x=[oat_min, green_threshold],\n    y=[oat_min, green_threshold],\n    mode='lines',\n    line=dict(color='green', width=2),\n    name='100% Outside Air Line (MAT = OAT)'\n))\n\n# Dynamic minimum outside air line and its control bands\nfig.add_trace(go.Scatter(\n    x=oat_range,\n    y=min_outside_air,\n    mode='lines',\n    line=dict(color='red', width=2),\n    name='Dynamic Minimum Outside Air Line'\n))\nfig.add_trace(go.Scatter(\n    x=oat_range,\n    y=min_outside_air_upper_band,\n    mode='lines',\n    line=dict(color='red', dash='dot', width=2),\n    name='Upper Control Band (+2\u00b0F)'\n))\nfig.add_trace(go.Scatter(\n    x=oat_range,\n    y=min_outside_air_lower_band,\n    mode='lines',\n    line=dict(color='red', dash='dot', width=2),\n    name='Lower Control Band (-2\u00b0F)'\n))\n\n# Final plot adjustments\nfig.update_layout(\n    title='AHU Economizer Analysis - Operation Chart',\n    xaxis_title='Outside Air Temperature (\u00b0F)',\n    yaxis_title='Mixed Air Temperature (\u00b0F)',\n    legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.7)'),\n    template='plotly_white'\n)\n\n# Show the plot\nfig.show()\n</code></pre>"},{"location":"examples/Data%20Visualization/airflow_sankey/","title":"Building Airflow by AHU and VAV","text":"<p>This example demonstrates the relationships, location, and changes related to airflow across different air systems within a building.</p>"},{"location":"examples/Data%20Visualization/airflow_sankey/#output","title":"Output","text":"\ud83d\udd0d Click for Fullscreen Show Code <pre><code>import networkx as nx\nfrom pyvis.network import Network\nimport pandas as pd\nfrom bdx.core import BDX\nfrom bdx.auth import UsernameAndPasswordAuthenticator\nfrom bdx.types import TimeFrame, AggregationLevel\nfrom bdx.components import ComponentFilter\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport matplotlib.colors as mcolors\n\n# BDX Credentials and Connection\nBDX_URL = \"http://yourURL.com\"  # Replace with your actual BDX URL\nUSERNAME = \"YOUR_USERNAME\"\nPASSWORD = \"YOUR_PASSWORD\"\nBUILDING_NAME = \"YOUR_BUILDINGNAME\"\n\n# Connect to BDX\nauth = UsernameAndPasswordAuthenticator(USERNAME, PASSWORD)\nwith BDX(BDX_URL, auth) as bdx:\n    buildings = bdx.buildings.list()\n    matching_buildings = [b for b in buildings if b.name.lower() == BUILDING_NAME.lower()]\n    if not matching_buildings:\n        print(f\"No building found with the name: {BUILDING_NAME}\")\n        exit()\n\n    BUILDING_ID = matching_buildings[0].componentInstanceId\n    BUILDING_NODE = f\"Building: {BUILDING_NAME}\"\n\n    print(f\"\\nSelected Building ID: {BUILDING_ID} for '{BUILDING_NAME}'\")\n\n    # Retrieve all components\n    all_components = bdx.components.by_building(building_id=BUILDING_ID)\n\n    # Define AHUs of interest\n    AHU_NUMBERS = [1, 2, 3, 4, 6, 8]\n    ahu_names = {f\"AHU_{num}\": f\"AHU {num}\" for num in AHU_NUMBERS}\n\n    # Prepare dict to hold AHU -&gt; list of VAV components\n    ahu_components = {ahu: [] for ahu in ahu_names}\n\n    # Manually filter for VAVs that belong to these AHUs\n    vav_components = [\n        comp for comp in all_components \n        if \"VAV_\" in comp.path.displayName \n        and any(comp.path.displayName.startswith(f\"VAV_{ahu}_\") for ahu in AHU_NUMBERS)\n    ]\n\n    # Map each VAV displayName -&gt; AHU_x\n    vav_to_ahu = {}\n    for vav in vav_components:\n        ahu_number = vav.path.displayName.split(\"_\")[1]  # Extract \"1\" from \"VAV_1_xxx\"\n        if f\"AHU_{ahu_number}\" in ahu_names:\n            vav_to_ahu[vav.path.displayName] = f\"AHU_{ahu_number}\"\n            ahu_components[f\"AHU_{ahu_number}\"].append(vav)\n\n    # Retrieve airFlow data for two timeframes\n    timeframe_current = TimeFrame.last_7_days()\n    timeframe_previous = TimeFrame.last_n_days(14)  # last 14 days, but we'll only compare the first 7 to the last 7\n\n    properties = [{\"componentPathId\": vav.path.componentPathId, \"propertyName\": \"airFlow\"} for vav in vav_components]\n\n    # Get current week data (7 days)\n    trend_data_current = bdx.trending.retrieve_data(properties, timeframe_current, AggregationLevel.HOURLY)\n    df_current = trend_data_current.dataframe.fillna(0).set_index(\"time\")\n\n    # Get previous week data (7 days before that)\n    trend_data_previous = bdx.trending.retrieve_data(properties, timeframe_previous, AggregationLevel.HOURLY)\n    df_previous = trend_data_previous.dataframe.fillna(0).set_index(\"time\")\n    # Trim previous to same length as current (assumes same # of hours)\n    df_previous = df_previous.iloc[: len(df_current)]\n\n    # Aggregate total airflow for both timeframes\n    total_airflow_current = df_current.sum().to_dict()   # e.g. {'compId_airFlow': totalCFM, ...}\n    total_airflow_previous = df_previous.sum().to_dict()\n\n# -----------------------------------------------------------------------------\n# 1) Compute all percent differences in one pass\n# -----------------------------------------------------------------------------\nall_percent_diffs = {}\nall_current_airflows = {}\n\nfor vav_comp in vav_components:\n    comp_id = vav_comp.path.componentPathId\n    current_airflow = total_airflow_current.get(f\"{comp_id}_airFlow\", 0)\n    previous_airflow = total_airflow_previous.get(f\"{comp_id}_airFlow\", 0)\n    if previous_airflow != 0:\n        percent_diff = ((current_airflow - previous_airflow) / previous_airflow) * 100\n    else:\n        percent_diff = 0\n\n    # Store these results by VAV displayName\n    all_percent_diffs[vav_comp.path.displayName] = percent_diff\n    all_current_airflows[vav_comp.path.displayName] = current_airflow\n\n# If everything is empty, avoid errors\nif len(all_percent_diffs) == 0:\n    vmin, vmax = -1, 1\nelse:\n    vmin = min(all_percent_diffs.values())\n    vmax = max(all_percent_diffs.values())\n\n# -----------------------------------------------------------------------------\n# 2) Create the TwoSlopeNorm and colormap once\n# -----------------------------------------------------------------------------\ncolormap = plt.get_cmap(\"RdBu_r\")  # Blue for negative, red for positive\nnorm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n\n# For consistent node sizing, get the max current airflow across all VAVs\nif len(all_current_airflows) == 0:\n    overall_max_airflow = 1\nelse:\n    overall_max_airflow = max(all_current_airflows.values())\n\n# -----------------------------------------------------------------------------\n# 3) Build the PyVis network\n# -----------------------------------------------------------------------------\nnet = Network(height=\"1000px\", width=\"100%\", notebook=True, directed=False)\n\n# Enable physics for dynamic spacing (avoids overlap)\nnet.barnes_hut(gravity=-7000, central_gravity=0.2, spring_length=50, spring_strength=0.03)\n\n# Add building node\nnet.add_node(\n    BUILDING_NODE, \n    size=100, \n    color=\"#3e3e3e\", \n    label=f\"Building: Apex Pavilion\", \n    physics=True,\n    font={\"size\": 50}\n)\n\n# Add AHU nodes &amp; connect them to the building\nfor ahu, ahu_label in ahu_names.items():\n    net.add_node(\n        ahu, \n        size=50, \n        color=\"#f5d76e\", \n        label=f\"P1_{ahu_label}\", \n        physics=True,\n        font={\"size\": 40}\n    )\n    net.add_edge(BUILDING_NODE, ahu)\n\n    # Get the VAV displayNames belonging to this AHU\n    ahu_vavs = [vav for vav, linked_ahu in vav_to_ahu.items() if linked_ahu == ahu]\n\n    # Sort them by absolute airflow change if you wish\n    # (We can derive airflow change from the stored percent or from the actual flows if needed.)\n    # For demonstration, let's just get the current minus previous from all_percent_diffs if needed.\n    # But you already had a dictionary \"airflow_differences\" if you want to re-use it.\n    # We'll do a quick inline approach:\n    def get_airflow_change(vav_disp_name):\n        # We can reconstruct from percent_diffs or better, do a direct sum again:\n        # But let's use the dictionary \"airflow_differences\" from your original code.\n        # If you still want that, we can compute it similarly:\n        comp = next((c for c in vav_components if c.path.displayName == vav_disp_name), None)\n        if comp is None:\n            return 0\n        comp_id = comp.path.componentPathId\n        cur_val = total_airflow_current.get(f\"{comp_id}_airFlow\", 0)\n        prev_val = total_airflow_previous.get(f\"{comp_id}_airFlow\", 0)\n        return (cur_val - prev_val)\n\n    sorted_vavs = sorted(\n        ahu_vavs,\n        key=lambda name: abs(get_airflow_change(name)),\n        reverse=True\n    )\n\n    # Loop through each VAV\n    for vav_disp_name in sorted_vavs:\n        # Grab the current airflow &amp; percent diff we stored\n        current_airflow = all_current_airflows.get(vav_disp_name, 0)\n        percent_diff = all_percent_diffs.get(vav_disp_name, 0)\n\n        # Convert the percent_diff to a color\n        rgba_color = colormap(norm(percent_diff))  # e.g. negative =&gt; bluish, positive =&gt; reddish\n        hex_color = mcolors.to_hex(rgba_color)\n\n        # Scale VAV size by current airflow\n        node_size = 20 + (50 * (current_airflow / max(1, overall_max_airflow)))\n\n        # Add VAV node\n        net.add_node(\n            vav_disp_name, \n            size=node_size, \n            color=hex_color,\n            title=f\"{vav_disp_name} - % Change: {percent_diff:.2f}%, Airflow: {current_airflow:.2f}\",\n            physics=True,\n            font={\"size\": 30}\n        )\n\n        # Add edge from AHU -&gt; VAV\n        net.add_edge(\n            ahu,\n            vav_disp_name,\n            width=1,\n            title=f\"% Change: {percent_diff:.2f}%\"\n        )\n\n# Generate and save the network visualization\nnet.show(\"VAV_network.html\")\n\nprint(\"\\n\u2705 Network visualization saved as 'VAV_network.html'. Open it in a browser to view.\")\n</code></pre>"},{"location":"examples/Data%20Visualization/mapenergychart/","title":"Total Energy Map by Building - Building Type","text":"\ud83d\udd0d Click for Fullscreen Show Code <p>Get energy data and sum/chart over map area <pre><code>    ### Use BDXpy here to call your buildings and load a gps file to match on building name #####\n    ##  store variables in df in below code\n\n\n    # Ensure your start and end dates are in datetime format\n    start_date = pd.to_datetime('2024-09-01')\n    end_date = pd.to_datetime('2024-09-30')\n\n    # Filter the DataFrame\n    filtered_df = df[(df['Timestamp'] &gt;= start_date) &amp; (df['Timestamp'] &lt;= end_date)]\n    filtered_df.to_csv(\"filtered_df.csv\", index=False)\n\n    # Define columns to keep without aggregation (taking the first occurrence)\n    cols_to_keep = [\"Building Name\", \"Building Type\", \"Latitude\", \"Longitude\"]\n\n    # Define columns to sum\n    cols_to_sum = [\"Total Energy\", \"Electric Energy\", \"Cooling Energy\", \"Heating Energy\",\n                \"Total Cost\", \"Metric Tons CO2\"]\n\n    # Group by 'Building Name', summing only the relevant columns\n    df_summed = filtered_df.groupby(\"Building Name\", as_index=False).agg(\n        {**{col: 'first' for col in cols_to_keep},  # Keep first occurrence\n        **{col: 'sum' for col in cols_to_sum},  # Sum numeric values\n        \"Timestamp\": \"count\"}  # Count total rows\n    )\n\n    # Rename the counted column\n    df_summed = df_summed.rename(columns={\"Timestamp\": \"Total Rows Summed\"})\n    df_summed.to_csv(\"df_summed.csv\", index=False)\n\n    # Create a map with bubble size variation and opacity\n    fig = px.scatter_mapbox(\n        df_summed,\n        lat='Latitude',\n        lon='Longitude',\n        hover_name='Building Name',\n        size='Total Energy',  # Bubble size based on Total Energy\n        color='Building Type',  # Color based on Building Type\n        size_max=40,  # Max bubble size\n        zoom=14,\n        mapbox_style=\"carto-positron\",\n        text='Building Name', \n        opacity = 0.6,\n        title= \"Energy Consumption by Building Type\"\n    )\n\n    # Set map layout\n    fig.update_layout(\n        mapbox_center={\"lat\": 51.7540, \"lon\": -1.2577},\n        margin={\"r\": 10, \"t\": 100, \"l\": 10, \"b\": 10},\n        height=800  # Adjust height for better layout\n    )\n\n    fig.update_traces(marker=dict(opacity=0.6))\n\n    fig.show()\n    print(fig.data[0])\n\n\n    # Example usage\n    fig.write_html(\"mapenergychart.html\")\n</code></pre></p>"},{"location":"examples/Data%20Visualization/operating_room_sankey_diagram/","title":"Hospital Network OR KPIs","text":"\ud83d\udd0d Click for Fullscreen Show Code <pre><code>    import plotly.graph_objects as go\n    import pandas as pd\n    import numpy as np\n    import matplotlib.cm as cm\n    import matplotlib.colors as mcolors\n\n    ### Use BDXpy here to call your buildings #####\n    ##  store variables in df in below code\n\n    # Define seed for reproducibility\n    np.random.seed(42)\n\n    # Define the structure of data\n    buildings = [\"General Hospital\", \"City Medical Center\", \"Westside Clinic\", \"Eastview Hospital\"]\n    air_handlers = {\n        \"General Hospital\": np.random.choice([\"GH_AHU1\", \"GH_AHU2\", \"GH_AHU3\"], size=np.random.randint(1, 4), replace=False).tolist(),\n        \"City Medical Center\": np.random.choice([\"CMC_AHU1\", \"CMC_AHU2\", \"CMC_AHU3\"], size=np.random.randint(1, 4), replace=False).tolist(),\n        \"Westside Clinic\": np.random.choice([\"WC_AHU1\", \"WC_AHU2\"], size=np.random.randint(1, 3), replace=False).tolist(),\n        \"Eastview Hospital\": np.random.choice([\"EVH_AHU1\", \"EVH_AHU2\", \"EVH_AHU3\"], size=np.random.randint(1, 4), replace=False).tolist()\n    }\n\n    operating_rooms = {}\n    for ahu in sum(air_handlers.values(), []):\n        num_ors = np.random.randint(2, 7)\n        operating_rooms[ahu] = [f\"{ahu}_OR{i+1}\" for i in range(num_ors)]\n\n    # Generate dummy data with updated air changes and runtime-energy cost correlation\n    data = []\n    out_of_range_high = 0\n    out_of_range_low = 0\n\n    for building in buildings:\n        for ahu in air_handlers[building]:\n            for or_room in operating_rooms[ahu]:\n                if out_of_range_high &lt; 3 and building == \"General Hospital\":\n                    air_changes = np.round(np.random.uniform(23, 28), 2)  # Out of range high\n                    out_of_range_high += 1\n                elif out_of_range_low &lt; 1 and building == \"City Medical Center\":\n                    air_changes = np.round(np.random.uniform(17, 19), 2)  # Out of range low\n                    out_of_range_low += 1\n                else:\n                    air_changes = np.round(np.random.uniform(20, 22), 2)  # Normal range\n\n                runtime = np.round(np.random.uniform(50, 168), 2)  # Weekly runtime in hours\n                energy_cost = np.round(runtime * air_changes * np.random.uniform(0.5, 1.5), 2)  # Scaled to runtime and air changes\n                temperature = np.round(np.random.uniform(60, 80), 2)  # Temperature in F\n                data.append([building, ahu, or_room, air_changes, runtime, energy_cost, temperature])\n\n    # Convert to DataFrame\n    columns = [\"Building\", \"AirHandler\", \"OperatingRoom\", \"AirChanges\", \"Runtime\", \"Cost\", \"Temperature\"]\n    df = pd.DataFrame(data, columns=columns)\n\n    # Aggregate data for Building -&gt; AHU links\n    ahu_totals = df.groupby([\"Building\", \"AirHandler\"]).agg({\n        \"AirChanges\": \"sum\",\n        \"Runtime\": \"sum\",\n        \"Cost\": \"sum\",\n        \"Temperature\": \"mean\"  # Avg temperature for AHU\n    }).reset_index()\n\n    # Order nodes for better readability\n    ordered_nodes = []\n    for b in buildings:\n        ordered_nodes.append(b)\n        for ahu in air_handlers[b]:\n            ordered_nodes.append(ahu)\n            ordered_nodes.extend(operating_rooms[ahu])\n\n    nodes_map = {node: i for i, node in enumerate(ordered_nodes)}\n\n    # Create links with multiple metric values\n    links = []\n    link_values = {\"AirChanges\": [], \"Runtime\": [], \"Cost\": [], \"Temperature\": []}\n    colors_map = {\"AirChanges\": [], \"Runtime\": [], \"Cost\": [], \"Temperature\": []}\n    plasma = cm.get_cmap(\"plasma\")\n\n    # Add Building -&gt; AHU links\n    for _, row in ahu_totals.iterrows():\n        links.append({\"source\": nodes_map[row['Building']], \"target\": nodes_map[row['AirHandler']]})\n        link_values[\"AirChanges\"].append(row['AirChanges'])\n        link_values[\"Runtime\"].append(row['Runtime'])\n        link_values[\"Cost\"].append(row['Cost'])\n        link_values[\"Temperature\"].append(row['Temperature'])\n        colors_map[\"AirChanges\"].append(\"gray\")  # Default color\n        colors_map[\"Runtime\"].append(\"gray\")\n        colors_map[\"Cost\"].append(\"gray\")\n        colors_map[\"Temperature\"].append(\"gray\")\n\n    # Add AHU -&gt; OR links with color scaling\n    for _, row in df.iterrows():\n        links.append({\"source\": nodes_map[row['AirHandler']], \"target\": nodes_map[row['OperatingRoom']]})\n        link_values[\"AirChanges\"].append(row['AirChanges'])\n        link_values[\"Runtime\"].append(row['Runtime'])\n        link_values[\"Cost\"].append(row['Cost'])\n        link_values[\"Temperature\"].append(row['Temperature'])\n\n        # Air Changes color scheme\n        if row['AirChanges'] &lt; 20:\n            colors_map[\"AirChanges\"].append(\"lightcoral\")  # Low side\n        elif row['AirChanges'] &gt; 22:\n            colors_map[\"AirChanges\"].append(\"firebrick\")  # High side\n        else:\n            colors_map[\"AirChanges\"].append(\"gray\")  # Neutral range\n\n        # Runtime and Cost color gradient using plasma colormap\n        norm_runtime = (row['Runtime'] - 50) / (168 - 50)  # Normalize runtime\n        norm_cost = (row['Cost'] - df['Cost'].min()) / (df['Cost'].max() - df['Cost'].min())\n        colors_map[\"Runtime\"].append(mcolors.to_hex(plasma(norm_runtime)))\n        colors_map[\"Cost\"].append(mcolors.to_hex(plasma(norm_cost)))\n        colors_map[\"Temperature\"].append(\"gray\")  # Keep neutral gray for temperature\n\n    # Create Sankey diagram with dropdown selection\n    fig = go.Figure()\n    metrics = [\"AirChanges\", \"Runtime\", \"Cost\", \"Temperature\"]\n    for metric in metrics:\n        fig.add_trace(go.Sankey(\n            visible=(metric == \"AirChanges\"),  # Default visible metric\n            node=dict(\n                pad=15,\n                thickness=20,\n                line=dict(color='black', width=0.5),\n                label=ordered_nodes\n            ),\n            link=dict(\n                source=[link['source'] for link in links],\n                target=[link['target'] for link in links],\n                value=link_values[metric],\n                color=colors_map[metric]  # Apply colors dynamically\n            )\n        ))\n\n    # Add dropdown menu centered above the chart\n    fig.update_layout(\n        title_text=\"Weekly KPIs - Operating Rooms\",\n        font_size=10,\n        updatemenus=[\n            {\n                \"buttons\": [\n                    {\"label\": metric, \"method\": \"update\", \"args\": [{\"visible\": [m == metric for m in metrics]}]} \n                    for metric in metrics\n                ],\n                \"direction\": \"down\",\n                \"showactive\": True,\n                \"x\": 0.5,\n                \"xanchor\": \"center\",\n                \"y\": 1.15,\n                \"yanchor\": \"top\",\n            }\n        ]\n    )\n\n    # Save to HTML\n    fig.write_html(\"operating_room_sankey_diagram.html\")\n    fig.show()\n</code></pre>"},{"location":"examples/Data%20Visualization/staticCurve/","title":"AHU Static Pressure Distribution Curve","text":"<p>This example demonstrates how to chart AHU static pressure distribution for a time period by AHU. Load your data from BDXpy, select a timeframe and revise any variables or chart formatting.</p>"},{"location":"examples/Data%20Visualization/staticCurve/#output","title":"Output","text":"Show Code <pre><code>    import pandas as pd\n    import numpy as np\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n\n    # Parameters for dummy data\n    np.random.seed(0)  # For reproducibility\n    num_ahus = 5\n    ahus = [f'AHU_{i+1}' for i in range(num_ahus)]\n    date_range = pd.date_range(start='2023-10-01', end='2023-10-31', freq='H')\n    num_samples = len(date_range)\n\n    # Generate random static pressure data with different transformations for each AHU\n    data = {\n        'datetime': np.tile(date_range, num_ahus),\n        'AHU': np.repeat(ahus, num_samples),\n        'static_pressure': []\n    }\n\n    # Apply different transformations to each AHU for variation\n    for i, ahu in enumerate(ahus):\n        if i == 0:  # AHU 1 - Flat line with slight random variation\n            pressures = np.random.uniform(1.5, 2.0, num_samples)\n        elif i == 1:  # AHU 2 - Logarithmic increase\n            pressures = np.logspace(0.1, 1, num_samples) / 10 + np.random.uniform(-0.1, 0.1, num_samples)\n        elif i == 2:  # AHU 3 - Polynomial curve (quadratic)\n            pressures = (np.linspace(1, 3, num_samples) ** 2) / 5 + np.random.uniform(-0.1, 0.1, num_samples)\n        elif i == 3:  # AHU 4 - Polynomial curve (cubic)\n            pressures = (np.linspace(1, 3, num_samples) ** 3) / 10 + np.random.uniform(-0.1, 0.1, num_samples)\n        else:  # AHU 5 - Linear with slight random variation and top values increased\n            pressures = np.linspace(1.0, 3.0, num_samples) + np.random.uniform(-0.1, 0.1, num_samples)\n            top_5_percent = int(0.05 * num_samples)\n            pressures[:top_5_percent] += np.linspace(0.5, 1.0, top_5_percent)  # Slight increase in top 5%\n\n        data['static_pressure'].extend(pressures)\n\n    # Create the DataFrame\n    df = pd.DataFrame(data)\n\n    # Plot the KDE curve using seaborn's displot\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(10, 6))\n    sns.displot(df, x='static_pressure', hue='AHU', kind=\"kde\", fill=True, height=6, aspect=1.5)\n\n    # Set title and adjust layout to avoid cutoff\n    plt.suptitle(\"Curve of Static Pressure Ranges by AHU\", y=1.05, fontsize=16)\n    plt.xlabel(\"Static Pressure (in w.c.)\")\n    plt.ylabel(\"Density\")\n    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adding padding for the title\n\n    # Save the plot as a PNG file\n    plt.savefig(\"AHU_static_pressure_kde_curve.png\", bbox_inches=\"tight\")\n    plt.show()\n</code></pre>"},{"location":"examples/Data%20Visualization/staticDuration/","title":"AHU Static Pressure Duration Comparison","text":"<p>This example demonstrates how to chart AHU static pressure duration for a time period by AHU. Load your data from BDXpy, select a timeframe and revise any variables or chart formatting.</p>"},{"location":"examples/Data%20Visualization/staticDuration/#output","title":"Output","text":"Show Code <pre><code>import pandas as pd\nimport numpy as np\nimport plotly.graph_objects as go\n\n# Parameters for dummy data\nnp.random.seed(0)  # For reproducibility\nnum_ahus = 5\nahus = [f'AHU_{i+1}' for i in range(num_ahus)]\ndate_range = pd.date_range(start='2023-10-01', end='2023-10-31', freq='H')\nnum_samples = len(date_range)\n\n# Generate random static pressure data with different transformations for each AHU\ndata = {\n    'datetime': np.tile(date_range, num_ahus),\n    'AHU': np.repeat(ahus, num_samples),\n    'static_pressure': []\n}\n\n# Apply different transformations to each AHU for variation\nfor i, ahu in enumerate(ahus):\n    if i == 0:  # AHU 1 - Flat line with slight random variation\n        pressures = np.random.uniform(1.5, 2.0, num_samples)\n    elif i == 1:  # AHU 2 - Logarithmic increase\n        pressures = np.logspace(0.1, 1, num_samples) / 10 + np.random.uniform(-0.1, 0.1, num_samples)\n    elif i == 2:  # AHU 3 - Polynomial curve (quadratic)\n        pressures = (np.linspace(1, 3, num_samples) ** 2) / 5 + np.random.uniform(-0.1, 0.1, num_samples)\n    elif i == 3:  # AHU 4 - Polynomial curve (cubic)\n        pressures = (np.linspace(1, 3, num_samples) ** 3) / 10 + np.random.uniform(-0.1, 0.1, num_samples)\n    else:  # AHU 5 - Linear with slight random variation and top values increased\n        pressures = np.linspace(1.0, 3.0, num_samples) + np.random.uniform(-0.1, 0.1, num_samples)\n        top_5_percent = int(0.05 * num_samples)\n        pressures[:top_5_percent] += np.linspace(0.5, 1.0, top_5_percent)  # Slight increase in top 5%\n\n    data['static_pressure'].extend(pressures)\n\n# Create the DataFrame\ndf = pd.DataFrame(data)\n\n# Function to generate the duration curve with binned percent exceedance for AHU static pressure\ndef generate_binned_duration_curve(df, ahus, bin_width=5):\n    duration_data = []\n    for ahu in ahus:\n        ahu_data = df[df['AHU'] == ahu].copy()\n        ahu_data = ahu_data.sort_values(by='static_pressure', ascending=False).reset_index(drop=True)\n\n        # Calculate percent exceedance and bin it\n        ahu_data['percent_exceedance'] = np.floor(np.arange(1, len(ahu_data) + 1) / len(ahu_data) * 100 / bin_width) * bin_width\n        # Group by binned percent_exceedance and calculate mean only on numeric columns\n        binned_ahu_data = ahu_data.groupby('percent_exceedance', as_index=False)['static_pressure'].mean()\n        binned_ahu_data['AHU'] = ahu  # Retain AHU identifier\n        duration_data.append(binned_ahu_data)\n    return pd.concat(duration_data, ignore_index=True)\n\n# Generate binned duration curve data\nduration_df = generate_binned_duration_curve(df, ahus, bin_width=5)\n\n# Plot the duration curve as overlapping bar charts with wider bins\ndef plot_binned_duration_curve(duration_df, ahus):\n    fig = go.Figure()\n    for ahu in ahus:\n        ahu_duration = duration_df[duration_df['AHU'] == ahu]\n        fig.add_trace(go.Bar(\n            x=ahu_duration['percent_exceedance'],\n            y=ahu_duration['static_pressure'],\n            name=ahu,\n            opacity=0.6\n        ))\n    fig.update_layout(\n        title=\"Binned Duration Curve for AHU Static Pressure (Overlapping Bar Charts)\",\n        xaxis_title=\"Percent Exceedance (%)\",\n        yaxis_title=\"Static Pressure (inches of water)\",\n        barmode='overlay',\n        legend_title=\"AHU\",\n        template=\"plotly_white\"\n    )\n    fig.show()\n\n# Run the plot function to save the file as HTML\nplot_binned_duration_curve(duration_df, ahus)\n</code></pre>"},{"location":"examples/LLM%20Integration/LLM/","title":"LLMs","text":"<p>Large Language Models (LLMs) can be used to summarize, interpret, and contextualize findings from BDX data. By processing HVAC and energy system data, LLMs can generate insights in a readable, action-oriented format, assisting building operators, facility managers, and analysts in making data-driven decisions.</p> <p>Example Use Case: VAV Air System Analysis There is an example using the OpenAI Python package to send BDX data for a VAV air system, prompting the model to generate a summary of airflow anomalies. The LLM can help highlight abnormal trends, suggest possible causes, and provide recommended actions based on the data.</p>"},{"location":"examples/LLM%20Integration/LLM/#using-bdxpy-with-openai-api","title":"Using BDXpy with OpenAI API","text":"<p>This guide explains how to use <code>bdxpy</code> with an OpenAI API key to analyze HVAC airflow data. The script retrieves airflow data from a BDX instance, detects anomalies, and generates a summary using OpenAI's API. Sending data to OpenAI is one piece that will be the focus of this example. But how you define data inputs, prompt engineering text, and select models is a crucial piece that each engineer or developer needs to review and account for.</p>"},{"location":"examples/LLM%20Integration/LLM/#openai-api-rate-limits","title":"OpenAI API Rate Limits","text":"<p>OpenAI imposes rate limits on API calls, which vary based on the model and subscription plan. As of recent updates, rate limits typically include: - GPT-4o &amp; GPT-4: Limited to a set number of requests per minute (RPM) and tokens per minute (TPM). - GPT-3.5: More relaxed limits but still subject to RPM and TPM constraints. - Free-tier users have significantly lower limits compared to API subscription plans.</p> <p>Refer to OpenAI\u2019s official rate limits documentation for the most up-to-date details.</p>"},{"location":"examples/LLM%20Integration/LLM/#why-sending-large-volumes-of-raw-bdx-data-is-not-recommended","title":"Why Sending Large Volumes of raw BDX Data is Not Recommended","text":"<p>The example script retrieves extensive airflow data from a BDX instance, but sending large datasets to OpenAI is inefficient due to: 1. API Rate Limits: Exceeding limits can result in throttling or failed requests. 2. High Token Costs: Large payloads consume more tokens, increasing costs. 3. Performance Delays: Processing large text blocks slows down response times and API calls.  </p>"},{"location":"examples/LLM%20Integration/LLM/#recommended-approach","title":"Recommended Approach:","text":"<ul> <li>Use OpenAI as a Framework: Instead of processing large volumes, focus on targeted statistics and smaller timeframes.  </li> <li>Pre-process Data Locally: Summarize key metrics (e.g., top anomalies, AHU-wide trends) before sending to OpenAI.  </li> <li>Batch Requests: Instead of one large request, break it into smaller, meaningful prompts.  </li> </ul>"},{"location":"examples/LLM%20Integration/LLM/#importance-of-prompt-engineering","title":"Importance of Prompt Engineering","text":"<p>Prompt engineering plays a critical role in obtaining useful outputs from OpenAI models.</p>"},{"location":"examples/LLM%20Integration/LLM/#key-strategies","title":"Key Strategies:","text":"<ul> <li>Be Specific: Provide context and constraints to guide responses.  </li> <li>Use Formatting Cues: Structure prompts using bullet points, tables, or numbered lists for better parsing.  </li> <li>Avoid Ambiguity: Clearly define what constitutes an anomaly or significant event in BDX data.  </li> <li>Iterate and Refine: Test different prompts to improve accuracy and relevance.  </li> </ul>"},{"location":"examples/LLM%20Integration/LLM/#model-selection-impact","title":"Model Selection Impact","text":"<p>Different OpenAI models produce varying results:</p> <ul> <li> <p>GPT-4o (Best for complex, structured data insights)  </p> </li> <li> <p>GPT-3.5 (Faster and cheaper but less accurate for nuanced analysis)  </p> </li> <li> <p>Fine-tuned Models (Can be trained on historical BDX data for better contextual responses)  </p> </li> </ul>"},{"location":"examples/LLM%20Integration/LLM/#choosing-the-right-model","title":"Choosing the Right Model","text":"Use Case Recommended Model Detailed anomaly detection GPT-4o General HVAC summaries GPT-3.5 Custom BDX optimizations Fine-tuned model"},{"location":"examples/LLM%20Integration/LLM/#final-recommendations","title":"Final Recommendations","text":"<ul> <li>Use OpenAI strategically to analyze key data points, not raw time-series data.  </li> <li>Fine-tune prompts to get actionable insights instead of general responses.  </li> <li>Optimize data selection before API calls to reduce costs and improve relevance.  </li> <li>Choose models based on complexity and budget.  </li> </ul> <p>By following these best practices, you can maximize OpenAI's value while efficiently leveraging BDX data for meaningful insights.</p>"},{"location":"examples/LLM%20Integration/LLM/#example-code","title":"Example Code","text":"<p>Below is example code where you can insert BDXpy code to generate chart on the left and an html summary of two difference responses back from OpenAI's API. Note: these are purely for API example purposes and need heavy modification for custom implementation elsewhere.</p> Show Code <pre><code>    import openai\n    import networkx as nx\n    from pyvis.network import Network\n    import pandas as pd\n    from bdx.core import BDX\n    from bdx.auth import UsernameAndPasswordAuthenticator\n    from bdx.types import TimeFrame, AggregationLevel\n    import matplotlib.pyplot as plt\n    import matplotlib.colors as mcolors\n    import json\n    import os\n    import markdown\n    from dotenv import load_dotenv\n\n    # Load environment variables\n    load_dotenv()\n\n    # OpenAI API Key\n    OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n    # BDX Credentials\n    BDX_URL = os.getenv(\"BDX_URL\")\n    USERNAME = os.getenv(\"BDX_USERNAME\")\n    PASSWORD = os.getenv(\"BDX_PASSWORD\")\n    BUILDING_NAME = \"Apex Building\" #building name to match component lookups on\n\n    # AHUs to lookup VAVs on per matching logic below\n    AHU_NUMBERS = [1, 2, 3, 4, 6, 8]\n\n    # Connect to BDX\n    auth = UsernameAndPasswordAuthenticator(USERNAME, PASSWORD)\n    with BDX(BDX_URL, auth) as bdx:\n        buildings = bdx.buildings.list()\n        matching_buildings = [b for b in buildings if b.name.lower() == BUILDING_NAME.lower()]\n        if not matching_buildings:\n            print(f\"No building found with the name: {BUILDING_NAME}\")\n            exit()\n\n        BUILDING_ID = matching_buildings[0].componentInstanceId\n        all_components = bdx.components.by_building(building_id=BUILDING_ID)\n\n        # Map AHUs\n        ahu_names = {f\"AHU_{num}\": f\"AHU {num}\" for num in AHU_NUMBERS}\n\n        # Filter for VAVs\n        vav_components = [\n            comp for comp in all_components\n            if \"VAV_\" in comp.path.displayName and any(comp.path.displayName.startswith(f\"VAV_{ahu}_\") for ahu in AHU_NUMBERS)\n        ]\n\n        # Map VAVs to AHUs\n        vav_to_ahu = {}\n        for vav in vav_components:\n            ahu_number = vav.path.displayName.split(\"_\")[1]\n            if f\"AHU_{ahu_number}\" in ahu_names:\n                vav_to_ahu[vav.path.displayName] = f\"AHU_{ahu_number}\"\n\n        # Retrieve airflow data for two timeframes\n        timeframe_current = TimeFrame.last_7_days()\n        timeframe_previous = TimeFrame.last_n_days(14)\n\n        properties = [{\"componentPathId\": vav.path.componentPathId, \"propertyName\": \"airFlow\"} for vav in vav_components]\n\n        # Fetch Data\n        trend_data_current = bdx.trending.retrieve_data(properties, timeframe_current, AggregationLevel.HOURLY)\n        trend_data_previous = bdx.trending.retrieve_data(properties, timeframe_previous, AggregationLevel.HOURLY)\n\n        df_current = trend_data_current.dataframe.fillna(0).set_index(\"time\")\n        df_previous = trend_data_previous.dataframe.fillna(0).set_index(\"time\")\n        df_previous = df_previous.iloc[:len(df_current)]\n\n        # Compute Percent Differences\n        anomalies = []\n        all_percent_diffs = {}\n        all_current_airflows = {}\n\n        for vav in vav_components:\n            comp_id = vav.path.componentPathId\n            display_name = vav.path.displayName\n\n            current_airflow = df_current.sum().get(f\"{comp_id}_airFlow\", 0)\n            previous_airflow = df_previous.sum().get(f\"{comp_id}_airFlow\", 0)\n\n            if previous_airflow != 0:\n                percent_diff = ((current_airflow - previous_airflow) / previous_airflow) * 100\n            else:\n                percent_diff = 0\n\n            all_percent_diffs[display_name] = percent_diff\n            all_current_airflows[display_name] = current_airflow\n\n            if abs(percent_diff) &gt; 20:\n                anomalies.append({\n                    \"VAV\": display_name,\n                    \"Current Airflow\": round(current_airflow, 2),\n                    \"Previous Airflow\": round(previous_airflow, 2),\n                    \"Change (%)\": round(percent_diff, 2)\n                })\n\n    # -------------------\n    # Generate Summary with OpenAI (Using GPT-4o)\n\n    # -------------------\n    def generate_summary(anomalies):\n        if not anomalies:\n            return \"&lt;p&gt;No significant anomalies detected in VAV airflow this week.&lt;/p&gt;\"\n\n    # Customize this prompt depending on your model, needs, performance of the response, etc. **** this prompt engineering is a very important step \n\n        prompt_text = f\"\"\"\n        Given the following data on airflow changes for VAVs in a building:\n        {json.dumps(anomalies, indent=2)}\n\n        ### **Summary Instructions**\n        - **Only report the most significant anomalies** (up to **5 individual VAVs**) OR if there is a **system-wide AHU issue** (total airflow of all VAVs under an AHU changes drastically).\n        - **Exclude moderate changes** \u2013 I only care about extreme cases that could indicate performance, comfort, or system inefficiencies.\n        - **If there are no significant changes**, state: \"No major anomalies detected this week.\"\n        - **Airflow data provided in an accumulation of CFM so units are CF\n        - **Format the response as concise bullet points**, using **Markdown formatting** for readability.\n\n        ### **Response Format**\n        - **Key Findings**  \n            - **VAV_3_3:** Airflow increased **+89.09%** \n            \ud83d\udd39 Likely cause: [Occupancy shift / Calibration issue / Setpoint change]  \n            \ud83d\udd39 Recommended action: [Verify control settings / Check mechanical operation]  \n\n        - **If AHU-wide issues exist, summarize them separately**  \n            - **AHU-1 System-Wide Change:** Total airflow increased by **+250,000 CF**, possibly due to [scheduling changes / pressure setpoint shift].  \n            \ud83d\udd39 Recommended action: [Check AHU damper settings / Review scheduling].  \n\n        Make sure the response is **short, direct, and action-oriented**.\n        \"\"\"\n\n        client = openai.OpenAI(api_key=OPENAI_API_KEY)\n\n    # if using a specific role/content/assistant in OpenAI make sure to correction specify in the client.chat.completions.create()\n\n        response = client.chat.completions.create(\n            model=\"gpt-4o\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are an expert in HVAC systems analyzing airflow changes.\"},\n                {\"role\": \"user\", \"content\": prompt_text}\n            ]\n        )\n\n        markdown_summary = response.choices[0].message.content\n        return markdown.markdown(markdown_summary)  \n\n    summary_text = generate_summary(anomalies)\n\n    # -------------------\n    # Generate PyVis Network Chart (With Hover Labels &amp; Correct Sizing)\n    # -------------------\n    html_path = \"VAV_network_summary_openai.html\"\n    net = Network(height=\"100vh\", width=\"100%\", notebook=True, directed=False)\n\n    # Maintain PyVis physics settings\n    net.barnes_hut(gravity=-7000, central_gravity=0.2, spring_length=50, spring_strength=0.03)\n\n    # Add Building Node\n    net.add_node(\"Building\", size=100, color=\"#3e3e3e\", label=f\"Building: {BUILDING_NAME}\", font={\"size\": 50})\n\n    # Add AHU Nodes\n    for ahu, ahu_label in ahu_names.items():\n        net.add_node(ahu, size=50, color=\"#f5d76e\", label=ahu_label, font={\"size\": 40})\n        net.add_edge(\"Building\", ahu)\n\n    # Determine maximum airflow for scaling\n    overall_max_airflow = max(all_current_airflows.values(), default=1)\n\n    # Add VAV Nodes with Hover Labels and Correct Sizing\n    colormap = plt.get_cmap(\"RdBu_r\")\n    vmin = min(all_percent_diffs.values(), default=-1)\n    vmax = max(all_percent_diffs.values(), default=1)\n    norm = mcolors.TwoSlopeNorm(vmin=vmin, vcenter=0, vmax=vmax)\n\n    for vav_name, ahu_name in vav_to_ahu.items():\n        percent_diff = all_percent_diffs.get(vav_name, 0)\n        current_airflow = all_current_airflows.get(vav_name, 0)\n\n        rgba_color = colormap(norm(percent_diff))\n        hex_color = mcolors.to_hex(rgba_color)\n\n        # Scale VAV size based on airflow\n        node_size = 5 + (50 * (current_airflow / overall_max_airflow))\n\n        net.add_node(\n            vav_name, \n            size=node_size, \n            color=hex_color, \n            title=f\"{vav_name} - % Change: {percent_diff:.2f}%, Airflow: {current_airflow:.2f}\",\n            font={\"size\": 30}\n        )\n        net.add_edge(ahu_name, vav_name, width=1)\n\n    # Save chart\n    net.save_graph(html_path)\n\n    # -------------------\n    # Write Final HTML\n    # -------------------\n    with open(html_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(f\"\"\"\n        &lt;html&gt;\n        &lt;head&gt;\n            &lt;style&gt;\n                body {{\n                    font-size: 12px; /* Base font size for body text */\n                }}\n                .container {{\n                    display: flex;\n                    height: 100vh;\n                    width: 100%;\n                }}\n                .left {{\n                    width: 50%;\n                    height: 100%;\n                    overflow-y: hidden; /* No scrollbar on left (chart) */\n                }}\n                .right {{\n                    width: 50%;\n                    background: #f4f4f4;\n                    padding: 20px;\n                    overflow-y: auto; /* Keep scrollbar on right (text) */\n                    font-size: 10px !important; /* Force body text size with !important */\n                    line-height: 1.5;\n                    box-sizing: border-box;\n                }}\n                /* Force smaller header sizes in .right with higher specificity */\n                .right h1 {{\n                    font-size: 16px !important; /* Smaller headline */\n                }}\n                .right h2 {{\n                    font-size: 14px !important; /* Smaller subhead */\n                }}\n                .right h3 {{\n                    font-size: 12px !important; /* Match body text size */\n                }}\n                /* Ensure all text in .right inherits the base size */\n                .right * {{\n                    font-size: 10px !important; /* Apply to all elements in .right */\n                }}\n            &lt;/style&gt;\n        &lt;/head&gt;\n        &lt;body&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;div class=\"left\"&gt;{net.generate_html()}&lt;/div&gt;\n                &lt;div class=\"right\"&gt;{summary_text}&lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/body&gt;\n        &lt;/html&gt;\n        \"\"\")\n\n\n    print(f\"Final version saved in '{html_path}'. Open in a browser.\")\n</code></pre>"}]}